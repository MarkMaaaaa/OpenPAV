{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"installation/","title":"Installation","text":"<p>To install and run OpenPAV, follow these steps:</p> <ol> <li> <p>Clone the repository:    <code>bash    git clone https://github.com/example/OpenPAV.git</code></p> </li> <li> <p>Install dependencies:    <code>bash    pip install -r requirements.txt</code></p> </li> <li> <p>Start the documentation server:    <code>bash    mkdocs serve</code></p> </li> </ol>"},{"location":"model-calibration/","title":"Model Calibration","text":"<p>OpenPAV supports calibration for various vehicle kinematic models, including IDM, Linear, and Machine Learning-based models. Detailed guides are provided in this section.</p>"},{"location":"overview/","title":"Overview","text":"<p>OpenPAV provides tools and datasets to simulate and study automated vehicle models. This documentation outlines the project goals, features, and how to use the platform effectively.</p>"},{"location":"quickstart/","title":"Quick Start","text":"<ol> <li>Select a vehicle model and algorithm using the interactive tool on the home page.</li> <li>Generate the parameter file by clicking 'Generate and Download'.</li> <li>Use the parameter file in your preferred simulation environment.</li> </ol>"},{"location":"simulation-integration/","title":"Simulation Integration","text":"<p>Learn how to integrate OpenPAV-generated parameter files into popular simulation platforms like SUMO and Vissim. Step-by-step instructions are available for each platform.</p>"},{"location":"vehicle-selection/","title":"Vehicle and Model Selection","text":"<p>Use the form below to select your vehicle and algorithm model. The generated file will be available for download.</p> Select Vehicle Model: --Select-- Tesla Audi Benz Select Algorithm Model: --Select-- Intelligent Driver Model (IDM) Linear Model Machine Learning Model Select Parameter File Type: --Select-- Original Parameters Vissim Parameters SUMO Parameters Generate and Download"},{"location":"md_files/codebase_structure/","title":"Codebase structure","text":""},{"location":"md_files/codebase_structure/#opencda-codebase-structure","title":"OpenCDA Codebase Structure","text":"<p>The <code>OpenCDA</code> codebase directory is structured as follows:</p> <pre><code>OpenCDA\n\u251c\u2500\u2500 docs  # documents of opencda, no need to pay attention.\n\u251c\u2500\u2500 opencda\n\u2502   \u251c\u2500\u2500 assests  # customized map and sumo xml package.\n\u2502   \u251c\u2500\u2500 co_simulation  # source codes for sumo background traffic generation.\n\u2502   \u251c\u2500\u2500 core  # the core part of the code\n\u2502   \u2502   \u2514\u2500\u2500 actutation # implementation of control algorithms\n\u2502   \u2502   \u251c\u2500\u2500 application # implementation of cooperative driving applications\n\u2502   \u2502   \u251c\u2500\u2500 common # base class and communication class for cavs\n\u2502   \u2502   \u251c\u2500\u2500 map # HDMap manager\n\u2502   \u2502   \u251c\u2500\u2500 plan # planning algorithms\n\u2502   \u2502   \u2514\u2500\u2500 sensing # perception adn localization algorithms.\n\u2502   \u251c\u2500\u2500 customize # where the users put their customized algorithm to replace the default modules\n\u2502   \u251c\u2500\u2500 scenario_testing # where all scenario testing scripts and yaml files exist\n\u2502   \u2502   \u2514\u2500\u2500 config_yaml # the yaml file defining the testing scenarios\n\u2502   \u2502   \u251c\u2500\u2500 evluations # evluation scripts for different modules' performance\n\u2502   \u2502   \u251c\u2500\u2500 utils # utility functions to construct scenarios based on given yaml files.\n\u251c\u2500\u2500 scripts  # installation scripts\n\u251c\u2500\u2500 tests  # unit tests\n</code></pre>"},{"location":"md_files/contributor/","title":"Contributor","text":""},{"location":"md_files/contributor/#about-us","title":"About Us","text":"<p>OpenCDA is brought to you by UCLA Mobility Lab.</p>"},{"location":"md_files/contributor/#supervisor","title":"Supervisor","text":"<p>Dr. Jiaqi Ma (Associate Professor @ UCLA) - Lab Principal Investigator - Linkedin - Google Scholar</p>"},{"location":"md_files/contributor/#core-developer","title":"Core Developer","text":"<p>Runsheng Xu (Ph.D. Student @ UCLA): - Project Lead and OpenCDA System Architect - Homepage - Linkedin</p> <p>Xu Han (Ph.D. Student @ UCLA): - Maps - Linkedin</p> <p>Hao Xiang (Ph.D. Student @ UCLA): - Trajectory Prediction, Functionality Enhancement - Homepage - Linkedin</p> <p>Dr. Xin Xia (Postdoc @ UCLA): - Cooperative Localization - Linkedin</p> <p>Dr. Yi Guo (Postdoc @ UC): - Platooning algorithm, SUMO Only Mode (under dev) - Linkedin</p>"},{"location":"md_files/customization/","title":"Customization","text":""},{"location":"md_files/customization/#algorithm-customization","title":"Algorithm Customization","text":"<p>Due to the high modularity of OpenCDA, you can conveniently replace any default module with your own algorithms. The best way for customization is to put your customized module under <code>opencda/customize/...</code> ,  and  use inheritance to overwrite the default algorithms. Afterwards, import your customized module in <code>VehicleManager</code> class. The only thing you need to pay attention is to make the input and output format the same as origin module if you only want to change a single module. Below we will show a detailed instruction  of customizations for each module.</p>"},{"location":"md_files/customization/#localization-customization","title":"Localization Customization","text":"<p>The class <code>LocalizationManager</code> takes charges of the localization task. During initialization, a gps and imu sensor will be spawned to collect localization related information. </p> <p>The core function in it is <code>localize()</code>, which doesn't take any input, and aims to fill the correct value for <code>self._ego_pos</code> and <code>_speed</code>. The default algorithm to fuse the gps and imu data is the Kalman Filter. It takes the current gps and imu data as input, and return the corrected <code>x, y, z</code> coordinates.</p> <pre><code>from opencda.core.sensing.localization.kalman_filter import KalmanFilter\n\nclass LocalizationManager(object):\n     def __init__(self, vehicle, config_yaml, carla_map):\n        self.kf = KalmanFilter(self.dt)\n\n     def localize(self):\n        ...\n        corrected_cords = self.kf(x, y, z, speed, yaw, imu_yaw_rate)\n</code></pre> <p>If a user wants to remain the whole structure of localization and just replace the filter (e.g. Extended Kalman Filter), then he/she just needs to create a <code>localization_manager.py</code> under <code>opencda/customize/core/sensing/localization</code> folder and initializes the <code>CustomizedLocalizationManager</code> with Extended Kalman Filter:</p> <pre><code>from opencda.core.sensing.localization.localization_manager import LocalizationManager\nfrom opencda.customize.core.sensing.localization.extented_kalman_filter import ExtentedKalmanFilter\n\nclass CustomizedLocalizationManager(LocalizationManager):\n    def __init__(self, vehicle, config_yaml, carla_map):\n        super(CustomizedLocalizationManager, self).__init__(vehicle, config_yaml, carla_map)\n        self.kf = ExtentedKalmanFilter(self.dt)\n</code></pre> <p>Then go to <code>VehicleManager</code> class, import this customized module and set it as the localizer.</p> <pre><code>from opencda.core.sensing.localization.localization_manager import LocalizationManager\nfrom opencda.customize.core.sensing.localization.localization_manager import CustomizedLocalizationManager\n\nclass VehicleManager(object):\n    def __init__(self, vehicle, config_yaml, application, carla_map, cav_world):\n        # self.localizer = LocalizationManager(vehicle, sensing_config['localization'], carla_map)\n        self.localizer = CustomizedLocalizationManager(vehicle, sensing_config['localization'], carla_map)\n</code></pre> <p>If the users want to modify more (e.g. change the data pre-processing), as long as they remember to fill <code>self._ego_pos</code> and <code>self._speed</code> in the <code>localize()</code> function under <code>CustomizedLocalizationManager</code>, it will not cause any problem for the downstream modules.</p>"},{"location":"md_files/customization/#perception-customization","title":"Perception Customization","text":"<p>The class <code>PerceptionManager</code> is responsible for perception related task. Right now it supports vehicle detection and traffic light detection. The core function <code>detect(ego_pos)</code> takes the ego position from localization module as the input, and return a dictionary <code>objects</code> whose keys are the object categories and the values are each object's attributes (e.g. 3d poses, static or dynamic) under world coordinate system in this category.</p> <p>To customize your own object detection algorithms, create a <code>perception_manager.py</code> under <code>opencda/customize/core/sensing/perception/</code> folder. </p> <pre><code>import cv2\nfrom opencda.core.sensing.perception.perception_manager import PerceptionManager\nfrom opencda.core.sensing.perception.obstacle_vehicle import ObstacleVehicle\nfrom opencda.core.sensing.perception.static_obstacle import TrafficLight\n\nclass CustomziedPeceptionManager(PerceptionManager):\n     def __init__(self, vehicle, config_yaml, cav_world, data_dump=False):\n        super(CustomizedLocalizationManager, self).__init__(vehicle, config_yaml, cav_world, data_dump)\n\n     def detect(self, ego_pos):\n        objects = {'vehicles': [],\n                   'traffic_lights': [],\n                   'other_objects_you_wanna_add' : []}\n\n\n        # retrieve current rgb images from all cameras\n        rgb_images = []\n        for rgb_camera in self.rgb_camera:\n            while rgb_camera.image is None:\n                continue\n            rgb_images.append(cv2.cvtColor(np.array(rgb_camera.image),\n                    cv2.COLOR_BGR2RGB))\n\n        # retrieve lidar data from the sensor\n        lidar_data = self.lidar.data\n\n        ########################################\n        # this is where you put your algorithm #\n        ########################################\n        objects = your_algorithm(rgb_images, lidar_data)\n        assert type(objects['vehicles']) == ObstacleVehicle\n        assert type(objects['traffic_lights']) == TrafficLight\n\n     return objects\n\n</code></pre>"},{"location":"md_files/customization/#behavior-planning-customization","title":"Behavior Planning Customization","text":"<p>During simulation runtime, <code>BehaviorAgent</code> first saves the ego vehicle position, speed and surrounding objects information from <code>PerceptionManager</code> and <code>LocalizationManager</code> through function <code>update_information</code>. Afterwards,  the <code>BehaviorAgent</code> will call function <code>run_step()</code> to execute a single step and return the <code>target_speed</code> and <code>target_location</code>.</p> <p>To customize your own behavior planning algorithms, create a <code>behavior_agent.py</code> under <code>opencda/customize/core/plan/</code> folder.</p> <pre><code>import carla.libcarla\nfrom opencda.core.plan.behavior_agent import BehaviorAgent\n\n\nclass CustomizedBehaviorAgent(BehaviorAgent):\n    def __init__(self, vehicle, carla_map, config_yaml):\n        ...\n\n    def update_information(self, ego_pos, ego_speed, objects):\n        ########################################\n        # this is where you put your algorithm #\n        ########################################\n        do_some_preprocessing(ego_pos, ego_speed, objects)\n\n    def run_step(self):\n        ########################################\n        # this is where you put your algorithm #\n        ########################################\n        target_speed, target_loc = your_plan_algorithm()\n        assert type(target_speed) == float\n        assert type(target_loc) == carla.Location\n\n        return target_speed, target_loc\n</code></pre>"},{"location":"md_files/customization/#control-customization","title":"Control Customization","text":"<p>Similar with <code>BehaviorAgent</code>, <code>ControlManager</code> first saves the ego vehicle and position through <code>update_info()</code>, and then takes the <code>target_speed</code> and <code>target_loc</code> generated from behavior agent to produce the final control command through <code>run_step()</code>.</p> <p>Different from other modules, <code>ControlManager</code> is more like a abstract class, which provides an interface to call the corresponding controller (default pid controller).</p> <pre><code>import importlib\n\nclass ControlManager(object):\n    def __init__(self, control_config):\n        controller_type = control_config['type']\n        controller = getattr(\n            importlib.import_module(\n                \"opencda.core.actuation.%s\" %\n                controller_type), 'Controller')\n        self.controller = controller(control_config['args'])\n\n    def update_info(self, ego_pos, ego_speed):\n        \"\"\"\n        Update ego vehicle information for controller.\n        \"\"\"\n        self.controller.update_info(ego_pos, ego_speed)\n\n    def run_step(self, target_speed, target_loc):\n        \"\"\"\n        Execute current controller step.\n        \"\"\"\n        control_command = self.controller.run_step(target_speed, waypoint)\n        return control_command\n</code></pre> <p>Therefore, if you want to use a controller other than pid controller, you can just create your <code>customize_controller.py</code> under <code>opencda/core/acutation/</code> folder, and follow the same input and output data format:</p> <pre><code>class CustomizeController:\n    def update_info(self, ego_pos, ego_spd):\n        ########################################\n        # this is where you put your algorithm #\n        ########################################\n        do_some_process(ego_pos, ego_spd)\n\n    def run_step(self, target_speed, target_loc):\n        ########################################\n        # this is where you put your algorithm #\n        ########################################\n        control_command = control(target_speed, target_loc)\n        assert control_command == carla.libcarla.VehicleControl\n        return control_command\n</code></pre> <p>Then put your controller's name in your yaml file:</p> <pre><code>vehicle_base:\n    controller:\n        type: customize_controller # this has to be exactly the same name as the controller py file\n        args: ......\n</code></pre>"},{"location":"md_files/developer_tutorial/","title":"Developer tutorial","text":""},{"location":"md_files/developer_tutorial/#class-design","title":"Class Design","text":"<p>In this section, we will take a deeper look at the implementation details of several important classes in our OpenCDA framework. For beginners, we encourage you to go through our OpenCDA Logic Flow first to understand the general simulation process. This tutorial will emphasize the detailed design logic behind each class and try to give clear descriptions of the core algorithms in each module. We will go through a test example <code>platoon_joining_2lanefree_carla.py</code> and for easy understanding, we have removed some non-core codes to simplify the code flow. To read the complete source code, please refer to our repo. For details about our cooperative architecture, please refer to our paper. </p> <p>Note: this tutorial assume you are using CARLA only instead of Co-simulation.</p>"},{"location":"md_files/developer_tutorial/#workflow","title":"Workflow","text":"<p>The workflow of opencda can be summarized as follows. </p> <ul> <li>Write yaml file to define various configurations of the simulation.</li> <li>Load those configurations into a dictionary <code>scenario_params</code>. For your convenience, we have provided <code>load_yaml</code> for loading the configurations. </li> <li>Create <code>CavWorld</code> object to store information of registered CAVs and more importantly, to store the shared large models like neural networks. </li> <li><code>ScenarioManager</code> will use those configurations to set <code>carla.TrafficManager</code> and load customized map if needed.</li> <li>After the creation of <code>ScenarioManager</code>, users can use its <code>create_vehicle_manager</code> method to spawn Connected Automated Vehicles (CAVs). Internally, each vehicle is managed by a <code>VehicleManager</code>, which will wrap the original <code>carla.Vehicle</code> with other modules such as perception, localization, control, and behavior agent. </li> <li>Besides single CAVs, there may also be platoons in the traffic. To spawn the platoons, users should use <code>create_platoon_manager</code> method. Similar to the previous single CAV, each member in the platoon will be managed by <code>VehicleManager</code>. In addition,  <code>PlatooningManager</code> is used to maintain the information of all the vehicles in the same platoon. And after the creation of vehicles and platoons, it will return a list of <code>PlatooningManager</code>. </li> <li>Similar to the process of spawning a single CAV, <code>create_traffic_carla</code> method will spawn background traffic with <code>autopilot=True</code>. </li> <li>Now all the CAVs and traffic flow are generated, thus we will enter the simulation loop. At the beginning of each simulation step, <code>scenario_manager.tick()</code> will be called to tick the server. Then all the CAVs will update the surrounding information and execute a single step. The single CAVs may join the platoon in the middle of the travel, so we need to check whether any single CAV has joined a platoon and remove it from the <code>single_cav_list</code> if so.</li> </ul> <pre><code>from opencda.scenario_testing.utils.customized_map_api import customized_map_helper\ndef run_scenario(opt, config_yaml):\n    scenario_params = load_yaml(config_yaml)\n    xodr_path = \"path/to/customized_map.xodr\"\n    # create CAV world\n    cav_world = CavWorld(opt.apply_ml)\n    # create scenario manager\n    scenario_manager = \\\n    sim_api.ScenarioManager(scenario_params,opt.apply_ml,xodr_path=xodr_path,cav_world=cav_world)\n    # create a list of single CAV\n    single_cav_list = \\\n    scenario_manager.create_vehicle_manager(['platooning'],map_helper=customized_map_helper)\n    # create platoon members\n    platoon_list = scenario_manager.create_platoon_manager(data_dump=False)\n    # create background traffic in carla\n    traffic_manager, bg_veh_list = scenario_manager.create_traffic_carla()\n    # run steps\n    while True:\n        scenario_manager.tick()\n        # update vehicles within platoon\n        for platoon in platoon_list:\n            platoon.update_information()\n            platoon.run_step()\n        # update signle CAV\n        for i, single_cav in enumerate(single_cav_list):\n            # If the CAV is merged into one of the platoon, then we should let platoon manage it.\n            # Thus we should remove them from single_cav_list\n            if single_cav.v2x_manager.in_platoon():\n                single_cav_list.pop(i)\n            # If the CAV is not contained in any platoon, then we should update the vehicle.\n            else:\n                single_cav.update_info()\n                control = single_cav.run_step()\n                single_cav.vehicle.apply_control(control)\n</code></pre>"},{"location":"md_files/developer_tutorial/#cavworld","title":"CavWorld","text":"<p><code>CavWorld</code>  stores CAVs and platoon information. Besides,  it will also store the shared machine learning models, so we don't need to store a separate model for each CAV. if you plan to use a neural network, this is a good place to load your model and call it in the customized algorithm class to utilize the model.</p> <pre><code>class CavWorld(object):\n    def __init__(self, apply_ml=False):\n        # store the set of carla.Vehicle ids.\n        self.vehicle_id_set = set()\n        # store (VehicleManager.vid, VehicleManager) pairs\n        self._vehicle_manager_dict = {}\n        # store (PlatooningManger.pmid, PlatooningManger) pairs\n        self._platooning_dict = {}\n        self.ml_manager = None\n        if apply_ml:\n            # add more ml models here\n            self.ml_manager = function_to_load_ML_model()\n</code></pre>"},{"location":"md_files/developer_tutorial/#scenariomanager","title":"ScenarioManager","text":"<p><code>ScenarioManager</code>  controls simulation construction, generates background traffic generation, and spawns CAVs. During the initialization stage, it will create the <code>client</code> and load the world with the HD map into the variable <code>self.world</code>. </p> <pre><code>class ScenarioManager:\n    def __init__(self, scenario_params,\n                   apply_ml,\n                   xodr_path=None,\n                   town=None,\n                   cav_world=None):\n        # create the carla.Client\n        self.client = carla.Client('localhost', simulation_config['client_port'])\n\n        # load customized map if any\n        self.world = load_customized_world(xodr_path, self.client)\n\n        # change the world setting\n        setting = self.world.get_settings()\n        setting.synchronous_mode = True\n        setting.fixed_delta_seconds = simulation_config['fixed_delta_seconds']\n\n        self.world.apply_settings(new_settings)\n        self.carla_map = self.world.get_map()\n</code></pre> <p>As we have seen in the workflow section, within this class, there are 3 important methods -- <code>create_vehicle_manager</code>, <code>create_platoon_manager</code>, and <code>create_traffic_carla</code>. </p> <p>The <code>create_vehicle_manager</code> method will create a list of single CAVs according to the configurations of the yaml files (like the destination, desired speed/limits, etc.).  The <code>create_platoon_manager</code> will create vehicles within platoons and create a platoon manager to group those vehicles. The <code>create_traffic_carla</code> will create background traffic in Carla and configure associated global TrafficManager behaviors. </p> <p>Now we will introduce each of them:</p> <ul> <li> <p><code>create_vehicle_manager()</code></p> <p>The CAVs information is stored as a list in the yaml file. Each entry in the list corresponds to a CAV's configuration.This method will pass spawn positions specified in the yaml file to the server and spawn the <code>carla.Vehicle</code> object. For each spawned vehicle, we will wrap it with the class <code>VehicleManager</code>, which essentially supports the localization, perception, platooning behavior, etc. Details about this class can be found in the  <code>VehicleManager</code> section. </p> <p><code>``python def create_vehicle_manager(self, application,                                map_helper=None,                                data_dump=False):     # By default, we use lincoln as our cav model.     cav_vehicle_bp = self.world.get_blueprint_library().find('vehicle.lincoln.mkz2017')     single_cav_list = []     # Each entry in the list corresponds to a CAV     for i, cav_config in enumerate(self.scenario_params['scenario']['single_cav_list']):         spawn_transform = function_to_load_spawn_position(cav_config)         cav_vehicle_bp.set_attribute('color', '0, 0, 255')         vehicle = self.world.spawn_actor(cav_vehicle_bp, spawn_transform)         # create vehicle manager for each cav         vehicle_manager = VehicleManager(vehicle, cav_config, application,...)         self.world.tick()         vehicle_manager.v2x_manager.set_platoon(None)                 # Set the destination of the vehicle according to the configuration         destination = carla.Location(x=cav_config['destination'][0],                                      y=cav_config['destination'][1],                                      z=cav_config['destination'][2])         # The</code>update_info` method will call the internal localization module and perception module         # to update position and detected objects.          # Those information is then again passed to the v2x_manager/controller/BehaviorAgent module.         vehicle_manager.update_info()         vehicle_manager.set_destination(vehicle_manager.vehicle.get_location(),destination,clean=True)</p> <pre><code>    single_cav_list.append(vehicle_manager)\n\nreturn single_cav_list\n</code></pre> <p>```</p> </li> <li> <p><code>create_platoon_manager()</code></p> <p>This method will first loop over the predefined platoon list. For each platoon, we will create a <code>PlatooningManager</code> object to group all the vehicles within the platoon. In the current version, we assume the first vehicle in the platoon is the lead vehicle. After creating all of the vehicles of each platoon, it will return a list of <code>PlatooningManager</code>. As a result, we can control the behavior of each platoon via a single line of code without worrying about details of how each vehicle will react. </p> <p><code>python def create_platoon_manager(self, map_helper=None, data_dump=False):         platoon_list = []     self.cav_world = CavWorld(self.apply_ml)     # we use lincoln as default choice since our UCLA mobility lab use the     # same car     cav_vehicle_bp = \\     self.world.get_blueprint_library().find('vehicle.lincoln.mkz2017')     # create platoons     for i, platoon in enumerate(self.scenario_params['scenario']['platoon_list']):         platoon_manager = PlatooningManager(platoon, self.cav_world)         for j, cav in enumerate(platoon['members']):             # Similar as spawning single CAV, we need to create its start location (spawn_transform)             # and set its color etc.              ...             vehicle = self.world.spawn_actor(cav_vehicle_bp,spawn_transform)             # create vehicle manager for each cav             vehicle_manager = VehicleManager(vehicle, cav, ['platooning'],                       self.carla_map, self.cav_world,                       current_time=self.scenario_params['current_time'],                       data_dumping=data_dump)             # add the vehicle manager to platoon             if j == 0:                 platoon_manager.set_lead(vehicle_manager)             else:                 platoon_manager.add_member(vehicle_manager, leader=False)             self.world.tick()         destination = carla.Location(x=platoon['destination'][0],                                      y=platoon['destination'][1],                                      z=platoon['destination'][2])         platoon_manager.set_destination(destination)         platoon_manager.update_member_order()         platoon_list.append(platoon_manager)     return platoon_list</code></p> </li> <li> <p><code>create_traffic_carla()</code>     This method will create the <code>carla.TrafficManager</code> and set associated parameters. Afterward, it will spawn the background vehicles. For spawning the vehicles, there are two options -- <code>spawn_vehicle_by_range</code> and <code>spawn_vehicles_by_list</code>. Depending on the way you configure them, the code will choose the associated one to do the task. Here for illustration, we use the <code>spawn_vehicles_by_list</code>. </p> <p>```python def create_traffic_carla(self):     traffic_config = self.scenario_params['carla_traffic_manager']     # get carla.TrafficManager     tm = self.client.get_trafficmanager()     tm.set_global_distance_to_leading_vehicle(       traffic_config['global_distance'])     tm.set_synchronous_mode(traffic_config['sync_mode'])     tm.set_osm_mode(traffic_config['set_osm_mode'])     tm.global_percentage_speed_difference(traffic_config['global_speed_perc'])</p> <pre><code>bg_list = spawn_vehicles_by_list(tm, traffic_config, bg_list)\n\nreturn tm, bg_list\n</code></pre> <p>```</p> <p>The <code>spawn_vehicles_by_list</code> has similar structure as <code>create_vehicle_manager</code> with the support of randomness of the vehicles' apperance and colors. Notice that, different from CAVs, we will set autopilot to <code>True</code> for those background traffic and we will return a list of <code>carla.Vehicle</code> instead of the <code>VehicleManager</code>  used in the <code>Create_vehicle_manager</code>. </p> </li> </ul>"},{"location":"md_files/developer_tutorial/#vehiclemanager","title":"VehicleManager","text":"<p>This class will wrap the original <code>carla.Vehicle</code> object and associate the vehicles with various modules including localization, perception, control, agent and V2Xmanager. </p> <pre><code>class VehicleManager(object):\n    def __init__(self, vehicle, config_yaml, application, \n                 carla_map, cav_world, current_time='',data_dumping=False):\n\n        # an unique uuid for this vehicle\n        self.vid = str(uuid.uuid1())\n        self.vehicle = vehicle\n        self.carla_map = carla_map\n\n        # retrieve the configure for different modules\n        sensing_config = config_yaml['sensing']\n        behavior_config = config_yaml['behavior']\n        control_config = config_yaml['controller']\n        v2x_config = config_yaml['v2x']\n\n        # v2x module\n        self.v2x_manager = V2XManager(cav_world, v2x_config)\n        # localization module\n        self.localizer = LocalizationManager(vehicle, sensing_config['localization'], carla_map)\n        # perception module\n        self.perception_manager = PerceptionManager(\n            vehicle, sensing_config['perception'], cav_world.ml_manager, data_dumping)\n        # BehaviorAgent\n        self.agent = BehaviorAgent(vehicle, carla_map, behavior_config)\n        # Control module\n        self.controller = ControlManager(config_yaml['controller'])\n        cav_world.update_vehicle_manager(self)\n</code></pre> <p>The localization module will spawn the location-related sensor actors such as GNSS and IMU. And within the localization module, it will use the Kalman Filter to keep track of the vehicle's location and speed. </p> <p>The perception module will spawn perception-related sensors such as cameras and LiDAR. If the ML model is applied (<code>self.active=True</code>), it will also detect the surrounding vehicles with the default Yolov5 detector. If the ML model is not applied (<code>self.active=False</code>), it will use server information directly and use the LiDAR to filter out vehicles out of the range. </p> <p>The agent module is a key component in our architecture. It will utilize the perception and localization information to produce the planned trajectory that should be passed into the downstream controller,  so that the desired destination can be reached.  There are two types of agents in our released codebase -- <code>BehaviorAgent</code> and <code>PlatooningBehaviorAgent</code>. <code>BehaviorAgent</code> is designed for the single-vehicle while <code>PlatooningBehaviorAgent</code> has special methods to deal with the platooning behaviors. We will talk more about those classes in their sections. </p> <p>There are several commonly used methods within this class. Here we will briefly talk about each of them. </p> <ul> <li> <p><code>set_destination</code> </p> <p>It will call the agent's <code>set_destination</code> method to set the destination and prepare the global planner and local planner. Please refer to the localization module for details.</p> </li> <li> <p><code>update_info</code></p> <p>It will call the localization module to get the latest position and velocity. Besides, it will call the perception module to get a list of obstacle vehicles. Afterward, it will update the <code>agent</code> and <code>controller</code> as well to inform them about those changes. </p> </li> <li> <p><code>run_step</code></p> <p>It will take in the <code>target_speed</code> and call <code>agent.run_step(target_speed)</code> to get the ideal speed and location that we want to reach in the next time frame. After that, it will pass the speed and location to the controller to generate the actual control command. And it will return the control command to the caller. </p> </li> <li> <p><code>destroy</code></p> <p>It will destroy the vehicle and associated sensors. </p> </li> </ul>"},{"location":"md_files/developer_tutorial/#perceptionmanager","title":"PerceptionManager","text":"<p><code>PerceptionManager</code> will spawn perception-related sensors including camera, LiDAR, and Semantic LiDAR. In this class, there are also many attributes for visualization. For code simplicity, we have removed them from our sample code here. </p> <pre><code>class PerceptionManager:\n    def __init__(self, vehicle, config_yaml, cav_world, data_dump=False):\n\n                self.activate = config_yaml['activate']\n        self.rgb_camera = []\n        mount_position = ['front', 'right', 'left']\n        for i in range(self.camera_num):\n            self.rgb_camera.append(CameraSensor(vehicle, mount_position[i]))\n        self.lidar = LidarSensor(vehicle, config_yaml['lidar'])\n        if data_dump:\n            self.semantic_lidar = SemanticLidarSensor(vehicle,\n                                                      config_yaml['lidar'])\n        # count how many steps have been passed\n        self.count = 0\n        # ego position\n        self.ego_pos = None\n\n        # the dictionary contains all objects\n        self.objects = {}\n</code></pre> <p>For the current version, the main function we provide is detection. And there are two important methods in this class -- <code>detect</code> and <code>retrieve_traffic_lights</code>.</p> <ul> <li> <p><code>detect</code></p> <p>It will detect surrounding vehicles by using specified model.  If <code>self.activate</code> flag is set, it will use the Yolov5 stored in the <code>CavWorld</code> to detect the obstacle vehicles. Otherwise, it will use the server information directly. </p> <p><code>python def detect(self, ego_pos):     self.ego_pos = ego_pos     objects = {'vehicles': [],     'traffic_lights': []}     if not self.activate:             objects = self.deactivate_mode(objects)     else:             objects = self.activate_mode(objects)     self.count += 1     return objects</code></p> </li> <li> <p><code>retrieve_traffic_lights</code> </p> <p>It will retrieve the traffic light states directly from the server. Thus in current version, we use ground truth to get the traffic light data. We may consider adding customized traffic light detection module in the next version. Researchers can also replace this method with their own traffic light detection algorithm by simply rewriting the following method. </p> <p><code>python def retrieve_traffic_lights(self, objects):     world = self.vehicle.get_world()     tl_list = world.get_actors().filter('traffic.traffic_light*')     objects.update({'traffic_lights': []})     for tl in tl_list:         distance = self.dist(tl)         if distance &lt; 50:             traffic_light = TrafficLight(tl.get_location(),                          tl.get_state())             objects['traffic_lights'].append(traffic_light)     return objects</code></p> </li> </ul>"},{"location":"md_files/developer_tutorial/#localizationmanager","title":"LocalizationManager","text":"<p>The <code>LocalizationManager</code> will spawn location-related sensors including GNSS and IMU. And it will use the Kalman Filter to keep track of cars' location and speed. Though we read the speed and yaw angle directly from the server, to simulate the real world's uncertainty, noise is also added to the data retrieved from the server. And user can control the noise level by changing the parameters like <code>speed_stddev</code> in <code>yaml</code> file. </p> <pre><code>class LocalizationManager(object):\n    def __init__(self, vehicle, config_yaml, carla_map):\n\n        self.vehicle = vehicle\n        self.activate = config_yaml['activate']\n        self.map = carla_map\n        self.geo_ref = self.map.transform_to_geolocation(\n            carla.Location(x=0, y=0, z=0))\n\n        # speed and transform and current timestamp\n        self._ego_pos = None\n        self._speed = 0\n\n        # history track\n        self._ego_pos_history = deque(maxlen=100)\n        self._timestamp_history = deque(maxlen=100)\n\n        self.gnss = GnssSensor(vehicle, config_yaml['gnss'])\n        self.imu = ImuSensor(vehicle)\n\n        # heading direction noise\n        self.heading_noise_std = \\\n            config_yaml['gnss']['heading_direction_stddev']\n        self.speed_noise_std = config_yaml['gnss']['speed_stddev']\n\n        self.dt = config_yaml['dt']\n        # Kalman Filter\n        self.kf = KalmanFilter(self.dt)\n\n\n</code></pre>"},{"location":"md_files/developer_tutorial/#behavioragent","title":"BehaviorAgent","text":"<p>The behavior agent will collect the information from the perception and localization module and use this information to produce the planned trajectory, which will be later passed to the controller.  There are two types of planner within the agent -- local planner and global planner. The global planner will generate the global path, considering the static map. The local planner will utilize the new perceived information to modify the global plan so that it can avoid dynamic obstacles and obey traffic rules. </p> <pre><code>class BehaviorAgent(object):\n    def __init__(self, vehicle, carla_map, config_yaml):\n        # Load various parameters \n        ...\n        self._sampling_resolution = config_yaml['sample_resolution']\n\n        # collision checker\n        self._collision_check = CollisionChecker(time_ahead=config_yaml['collision_time_ahead'])\n\n        # used to indicate whether a vehicle is on the planned path\n        self.hazard_flag = False\n\n        # route planner related\n        self._global_planner = None\n        self.start_waypoint = None\n        self.end_waypoint = None\n\n        # intersection agent related\n        self.light_state = \"Red\"\n        self.light_id_to_ignore = -1\n\n        # trajectory planner\n        self._local_planner = LocalPlanner(self, carla_map, config_yaml['local_planner'])\n        # special behavior rlated\n        self.car_following_flag = False\n        # lane change allowed flag\n        self.lane_change_allowed = True\n        # destination temp push flag\n        self.destination_push_flag = False\n        # white list of vehicle managers that the cav does not consider as\n        # obstacles\n        self.white_list = []\n        self.obstacle_vehicles = []\n</code></pre> <ul> <li> <p><code>update_information</code>     The <code>VehicleManager</code> will call <code>agent.update_information</code> to update the position and speed as well as the detected objects. Afterward, the agent will update the local planner with the new speed/location information. For the detected objects, it may contain the vehicles that are about to join the platooning and those vehicles should be managed by the platooning manager. Thus we should remove those vehicles from the <code>objects</code> like shown below. Besides, we will also update the traffic light state here. </p> <p>```python def update_information(self, ego_pos, ego_speed, objects):     # update localization information     self._ego_speed = ego_speed     self._ego_pos = ego_pos     self.break_distance = self._ego_speed / 3.6 * self.emergency_param</p> <pre><code># update the localization info to trajectory planner\nself.get_local_planner().update_information(ego_pos, ego_speed)\n# The white list contains all position of target platoon member for joining. \n# Those vehicles should be managed by platooning manager. Thus we should remove them.\n# Right now the white_list_match function will associate the vehicles \n# based on their lane_id and location.\nobstacle_vehicles = objects['vehicles']\nself.obstacle_vehicles = self.white_list_match(obstacle_vehicles)\n\n# update the debug helper\nself.debug_helper.update(ego_speed, self.ttc)\nif self.ignore_traffic_light:\n    self.light_state = \"Green\"\nelse:\n    # This method also includes stop signs and intersections.\n    self.light_state = str(self.vehicle.get_traffic_light_state())\n</code></pre> <p>```</p> </li> <li> <p><code>set_destination</code></p> <p>Given the start location and end location, it will find the closest waypoints in the map to each of them. And we will use those 2 waypoints as the starting and end node. We will use the following code to make sure the starting node is always in front of the vehicle.</p> <p><code>python _, angle = cal_distance_angle(self.start_waypoint.transform.location, cur_loc, cur_yaw) while angle &gt; 90:     self.start_waypoint = self.start_waypoint.next(1)[0]     _, angle = cal_distance_angle(     self.start_waypoint.transform.location, cur_loc, cur_yaw)</code></p> <p>And we will call <code>self._trace_route</code> to find the list of waypoints from the starting node to the end node. Then, we will store the route into the buffer <code>self.waypoints_queue</code>. </p> </li> <li> <p><code>_trace_route</code></p> <p>This method will find the path from the <code>start_waypoint</code> to the <code>end_waypoint</code>. If the global plan has not been set before, it will load the <code>GlobalRoutePlanner</code> first. The algorithm for searching can be summarized as follows.</p> <ul> <li>Given the <code>start_waypoint</code>, find the edge in the original graph containing this waypoint's location (achieved through <code>GlobalRoutePlanner._localize</code>).  And use the start node of the edge as the actual search origin. Through the same process, we can find the actual destination based on the provided <code>end_point</code>. </li> <li>Use A* algorithm to find the path from origin to destination.  The path will contain a list of waypoints. The waypoint is either the start or end of a lane segment. Add the destination at the end of the path.</li> <li>For each of the traversed edges in the path, we can find the corresponding turn decision (it is called RoadOption, e.g. lanefollow, left, right turn, etc.). See <code>RoadOption</code> class for complete definition. Then, loop over the edges,<ul> <li>If the turn decision of the edge is lanefollow or void, then also add the <code>edge['path']</code> with associated RoadOption to the path.</li> <li>Else, only add current_waypoint and a point in the target lane with a certain distance away. (see <code>GlobalRoutePlanner.trace_route</code> for details.)</li> </ul> </li> <li>Now we have a list of (waypoint, RoadOption) pairs. </li> <li>Add the pairs to the <code>waypoints_queue</code>. If the <code>clean</code> flag is on, also  update the <code>_waypoint_buffer</code>. </li> </ul> </li> <li> <p><code>run_step</code> </p> <p>This method contains the function of behavior regulation and trajectory generation. To obey the traffic rules and consider the dynamic road elements, we have designed the following cases. Each case will have distinct driving behavior.</p> <ul> <li> <p>Destination arrived</p> <p>If the current location is near a radius of the destination ($10$ meters by default), the vehicle has arrived at the destination and we will exit the agent. </p> </li> <li> <p>Red traffic light</p> <p>If the vehicle is in the junction and the traffic light is red, then return a target speed of 0.  Here we also consider the case when the car has moved to the center of the junction and the traffic light turns green at the current timestamp. In this case, it is very dangerous for the car to stop at the center of the junction. Thus we will use <code>light_id_to_ignore</code> to ignore this red light so that the car will continue moving. See code for details</p> </li> <li> <p>Lane change behaviors</p> <ul> <li>If the car is near the intersection, for safety concerns, the overtake and lane change are not allowed.</li> <li>If the curvature of the road is high, the lane change is disabled. </li> <li>If a lane change is allowed and the global plan indeed outputs a lane change path, then do a collision check to see if the target lane is free. </li> </ul> </li> <li> <p>Car following</p> <p>If the car following mode is on, follow the leading car.</p> </li> <li> <p>Normal Mode</p> <p>For the normal mode, we will sample points along the path and return the target speed and target location. </p> </li> </ul> </li> </ul>"},{"location":"md_files/developer_tutorial/#v2xmanager","title":"V2XManager","text":"<p><code>V2XManager</code> is used to receive information from other CAVs and deliver ego information to them. The class attributes <code>ego_pos</code> and <code>ego_spped</code> are both deque type. Such data type is used to simulate the signal lagging during communication, e.g., <code>ego_pos[-3]</code> represents there is a lag of 2 time steps (<code>ego_pos[-1]</code> represents the most recent one).</p> <pre><code>class V2XManager(object):\n    def __init__(self, cav_world, config_yaml, vid):\n        # if disabled, no cooperation will be operated\n        self.cda_enabled = config_yaml['enabled']\n        self.communication_range = config_yaml['communication_range']\n\n        # found CAVs nearby\n        self.cav_nearby = {}\n\n        # used for cooperative perception.\n        self._recieved_buffer = {}\n\n        # used for platooning communication\n        self.platooning_plugin = PlatooningPlugin(\n            self.communication_range, self.cda_enabled)\n\n        self.cav_world = weakref.ref(cav_world)()\n\n        # ego position buffer. use deque so we can simulate lagging\n        self.ego_pos = deque(maxlen=100)\n        self.ego_spd = deque(maxlen=100)\n        # used to exclude the cav self during searching\n        self.vid = vid\n\n        # check if lag or noise needed to be added during communication\n        self.loc_noise = 0.0\n        self.yaw_noise = 0.0\n        self.speed_noise = 0.0\n        self.lag = 0\n\n        if 'loc_noise' in config_yaml:\n            self.loc_noise = config_yaml['loc_noise']\n        if 'yaw_noise' in config_yaml:\n            self.yaw_noise = config_yaml['yaw_noise']\n        if 'speed_noise' in config_yaml:\n            self.speed_noise = config_yaml['speed_noise']\n        if 'lag' in config_yaml:\n            self.lag = config_yaml['lag']\n</code></pre> <ul> <li><code>update_info</code></li> </ul> <p>This method updates the ego vehicle's speed and position and passes the updated information to different application plugins. Also, this method searches all of the neighboring vehicles within range. To search the vehicles, we need to retrieve a list of registered vehicles' information from <code>CavWorld</code>. For each vehicle in the list, we compute its distance to the ego vehicle. If the distance is less than the threshold (<code>communication_range</code>), we consider it as a neighboring vehicle.  </p> <p>```python     def update_info(self, ego_pos, ego_spd):         self.ego_pos.append(ego_pos)         self.ego_spd.append(ego_spd)         self.search()</p> <pre><code>    # the ego pos in platooning_plugin is used for self-localization,\n    # so we shouldn't add noise or lag.\n    self.platooning_plugin.update_info(ego_pos, ego_spd)\n</code></pre> <p>```</p> <ul> <li><code>get_ego_pos</code> </li> </ul> <p>This method adds noise and lags to the ego pose and then delivers to other CAVs.</p> <p>```python     def get_ego_pos(self):          # add lag         ego_pos = self.ego_pos[0] if len(self.ego_pos) &lt; self.lag else \\             self.ego_pos[np.random.randint(-1 - int(abs(self.lag)), 0)]</p> <pre><code>    x_noise = np.random.normal(0, self.loc_noise) + ego_pos.location.x\n    y_noise = np.random.normal(0, self.loc_noise) + ego_pos.location.y\n    z = ego_pos.location.z\n    yaw_noise = np.random.normal(0, self.yaw_noise) + ego_pos.rotation.yaw\n\n    noise_location = carla.Location(x=x_noise, y=y_noise, z=z)\n    noise_rotation = carla.Rotation(pitch=0, yaw=yaw_noise, roll=0)\n\n    processed_ego_pos = carla.Transform(noise_location, noise_rotation)\n\n    return processed_ego_pos\n</code></pre> <p>```</p>"},{"location":"md_files/getstarted/","title":"Getstarted","text":""},{"location":"md_files/getstarted/#quick-start","title":"Quick Start","text":"<p>OpenCDA provides benchmark scenarios that researchers can use directly without any modification. We will also be adding new testing scenarios in future releases. Running these  scenario testings only need one line of command:</p> <pre><code>cd ~/OpenCDA\npython opencda.py -t scenario_name -v carla_version --apply_ml --record\n</code></pre> <p>Parameters explanation: * <code>-t</code>: The name of the tesitng scenario. A python script with the same name should exist in <code>opencda/scenario_testing/</code> to construct the simulation, and a yaml file with the same name should exist in  <code>opencda/scenario_testing/config_yaml/</code> to define the simulation parameters. * <code>-v --version</code>: Define the CARLA simulator version, default 0.9.11. Right now only 0.9.11 and 0.9.12 are supported.   We need this as there are some api changes from 0.9.11 to 0.9.12. * <code>--apply_ml</code>  (Optional): A flag to indicate whether a deep learning model needs to be loaded. If this flag is  set, Pytorch will be imported. * <code>--record</code> (Optional): A flag to indicate whether to record this simulation. Check here for more details.</p> <p>Below we will demonstrate some examples of running the benchmark testings in OpenCDA.</p>"},{"location":"md_files/getstarted/#single-vehicle-test","title":"Single Vehicle Test","text":"<p>**Notice:  Change -v 0.9.11 to -v 0.9.12 if you are using CARLA 0.9.12.</p>"},{"location":"md_files/getstarted/#1-two-lane-highway-test","title":"1. Two-lane highway test","text":"<pre><code>python opencda.py -t single_2lanefree_carla -v 0.9.11\n</code></pre> <p>In this scenario , a single Connected Automated Vehicle will be spawned at a 2-lane highway customized map.  This CAV will try to reach the assigned destination with a desired speed of 100km/h and manage to safely interact with the surrounding traffic flow. The CAV's localization, planning, and control modules will be activated, and the perception module will be deactivated by default, thus  pytorch is NOT required in this testing . </p> <p>If you want to activate the perception module, please check Yaml Defining Rules to see details.</p> <p></p> <p>Note: The bounding boxes draw on the camera and lidar are retrieved from the server directly and   projected to the sensor space</p>"},{"location":"md_files/getstarted/#2-town06-test-pytorch-required","title":"2. Town06 test (Pytorch required)","text":"<pre><code>python opencda.py -t single_town06_carla  -v 0.9.11 --apply_ml\n</code></pre> <p>The apply_ml flag will import the pytorch library and load Yolov5 model (Thus Pytorch is required) for object detection. Thus, in this scenario, the perception, localization, planning and control modules will all be activated. </p> <p>Note: The bounding box draw here comes from the detection model.</p>"},{"location":"md_files/getstarted/#3-town06-co-simulation-test-pytorch-and-sumo-required","title":"3. Town06 Co-simulation test (Pytorch and Sumo required)","text":"<pre><code>python opencda.py -t single_town06_cosim  -v 0.9.11 --apply_ml\n</code></pre> <p>This scenario applies Sumo to generate the traffic flow instead of using Carla traffic manager. Yolov5 and simple Lidar fusion are used to detect object 3D bounding box. Therefore, both Pytorch and Sumo are required to install to run this benchmark test. </p>"},{"location":"md_files/getstarted/#cooperative-driving-test","title":"Cooperative Driving Test","text":""},{"location":"md_files/getstarted/#1-platooning-stability-test","title":"1. Platooning stability test","text":"<pre><code>python opencda.py -t platoon_stability_2lanefree_carla -v 0.9.11\n</code></pre> <p>In this scenario, a platoon with 4 members will be placed at the 2-lane highway map. The platoon leader will dramatically increases and decreases its speed to test whether the members can still keep the desired time gap. In this way, the platoon stability is verified.</p>"},{"location":"md_files/getstarted/#2-cooperative-merge-and-joining-a-platoon","title":"2. Cooperative merge and joining a platoon","text":"<pre><code>python opencda.py -t platoon_joining_2lanefree_carla -v 0.9.11\n</code></pre> <p>In this scenario, a platoon will drive on the mainline together with a mixed traffic flow. A single CAV will come from the  merging lane, communicate with the platoon to cooperatively merge into the mainline, and simultaneously join the platoon.</p> <p></p>"},{"location":"md_files/getstarted/#3-cooperative-merge-and-joining-a-platoon-under-co-simulation-sumo-required","title":"3. Cooperative merge and joining a platoon under co-simulation (SUMO required)","text":"<pre><code>python opencda.py -t platoon_joining_2lanefree_cosim -v 0.9.11\n</code></pre>"},{"location":"md_files/getstarted/#4-platoon-back-join-pytorch-required","title":"4. Platoon back-join (Pytorch required)","text":"<pre><code>python opencda.py -t platoon_joining_town06_carla  -v 0.9.11 --apply_ml\n</code></pre> <p>A single CAV will try to overtake several human-driven vehicles to join the platoon from the back. Pytorch is required for this test since we use yolov5 detetion. </p>"},{"location":"md_files/index-mkdocs/","title":"Welcome to OpenCDA","text":"<p>OpenCDA is a generalized framework for fast developing and testing cooperative driving automation  applications(e.g., cooperative perception, platooning) as well as autonomous vehicle components(e.g.,  perception, localization, planning, control) on multi-resolution simulators(e.g., CARLA, SUMO, NS3).</p> <p>OpenCDA is a work in progress, and many features are still in the future roadma.  We welcome your contribution and please visit out Github repo for latest release.</p>  OpenCDA Github"},{"location":"md_files/index-mkdocs/#contents","title":"Contents:","text":"<ul> <li>Introduction</li> <li>Installation</li> <li>Quick Start</li> <li>Tutorial <ul> <li>Codebase structure</li> <li>Step1: Define the yaml file</li> <li>Step2: Construct the scenario</li> <li>Step3: Execute a single step</li> <li>Step4: Run simulation loop</li> <li>Step5: Evaluation</li> <li>Customize your own modules</li> </ul> </li> <li>PythonAPI</li> </ul> <p>If you use OpenCDA for academic research, you are highly encouraged to cite our paper: <code>bibtex @inproceedings{xu2021opencda, title={OpenCDA: An Open Cooperative Driving Automation Framework Integrated with Multi-resolution Simulations}, author={Runsheng Xu, Yi Guo, Xu Han, Xin Xia, Jiaqi Ma}, booktitle={2021 IEEE Intelligent Transportation Systems Conference (ITSC)}, year={2021} }</code></p>"},{"location":"md_files/installation/","title":"Installation","text":"<ul> <li>System/Hardware Requirements</li> <li> <p>Local Installation</p> <ul> <li> <p>1. CARLA installation</p> <ul> <li>1.1 Package installation </li> <li>1.2 Build from source </li> </ul> </li> <li> <p>2. Install OpenCDA</p> </li> <li>3. Install Pytorch and Yolov5 (Optional)</li> <li>4. Install Sumo (Optional)</li> </ul> </li> <li> <p>Docker Installation</p> </li> </ul>"},{"location":"md_files/installation/#requirements","title":"Requirements","text":"<p>To get started, the following requirements should be fulfilled. * System requirements. Any 64-bits OS should run OpenCDA. We highly recommends Ubuntu  16.04/18.04/20.04.</p> <ul> <li>Adequate GPU. CARLA is a realistic simulation platform based on Unreal Engine 4, which requires at least a 3GB GPU for smooth scene rendering, though 8GB is recommended.</li> <li>Disk Space. Estimate 100GB of space is recommended to install CARLA and Unreal Engine. </li> <li>Python Python3.7 or higher version is required for full functions.</li> </ul>"},{"location":"md_files/installation/#local-installation","title":"Local Installation","text":"<p>To get OpenCDA v0.1 running with complete functionality, you will need four things: CARLA, OpenCDA, and Pytorch (optional). Pytorch is required only when you want to activate the perception module; otherwise OpenCDA will retrieve all perception information from the simulation server directly.</p>"},{"location":"md_files/installation/#1-carla-installation-0911-required","title":"1. CARLA Installation (&gt;= 0.9.11 required)","text":"<p>There are two different recommended ways to install the CARLA simulator and either way is fine for using OpenCDA.  Note: If you want to use the customized highway map with full assets (.fbx, .xml and .xodr) in OpenCDA,  you have to build from source. Visit CARLA's tutorial ADD a new map for more information.</p>"},{"location":"md_files/installation/#11-package-installation","title":"1.1 Package Installation","text":"<p>  CARLA Released Package </p> <p>OpenCDA is tested both at 0.9.11 and 0.9.12. To install CARLA as a precompiled package, download and extract the release file. It contains a precompiled version of the simulator, the Python API module and some scripts to be used as examples. </p> <p>Note: The  AdditionalMaps_0.9.1x.tar.gz also need to be downloaded and extract to the CARLA repo to support scenario testings in Town06.</p>"},{"location":"md_files/installation/#12-build-from-source","title":"1.2 Build From Source","text":"<p>For advanced CARLA usage that involves extensive customizations, Build CARLA from Source is also supported by OpenCDA. Though source build in  Windows OS is supported by CARLA, Ubuntu is the preferred OS as the OpenCDA was developoed in Ubuntu 18.04.  </p> <p>Note: OpenCDA do not require CARLA source build. However, customized map with building/lane/traffic light/road surface materials assets  in CARLA  require source build.  Visit CARLA's tutorial ADD a new map for more information. </p>"},{"location":"md_files/installation/#2-opencda-installation","title":"2. OpenCDA Installation","text":"<p>First, download OpenCDA github to your local folder if you haven't done it yet.</p> <pre><code>git clone https://github.com/ucla-mobility/OpenCDA.git\ncd OpenCDA\n</code></pre> <p>Make sure you are in the root dir of OpenCDA, and next let's install the dependencies. We highly recommend use conda environment to install. </p> <pre><code>conda env create -f environment.yml\nconda activate opencda\npython setup.py develop\n</code></pre> <p>If conda install failed,  install through pip</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>After dependencies are installed, we need to install the CARLA python library into opencda conda environment. You can do this by running this script:</p> <pre><code>export CARLA_HOME=/path/to/your/CARLA_ROOT\nexport CARLA_VERSION=0.9.11 #or 0.9.12 depends on your CARLA\n. setup.sh\n</code></pre> <p>If everything works correctly, you will see a cache folder is created in your OpenCDA root dir, and the terminal shows \"Successful Setup!\". To double check the carla package is correctly installed, run the following command and  there should be no error.</p> <pre><code>python -c \"import carla\" # check whether carla is installed correctly.\n</code></pre> <p>Note: If you are using Python other than 3.7 and CARLA &lt; 0.9.11 then you have to change the setup.sh to your carla version's egg file or manually installed carla to your conda environment.</p>"},{"location":"md_files/installation/#3-install-pytorch-and-yolov5-optional","title":"3. Install Pytorch and Yolov5 (Optional)","text":"<p>This section is only needed for the users who want to test perception algorithms. By default, OpenCDA does not require pytorch installed and it retrieves the object positions from the server directly. Once perception module is activated, then OpenCDA will use yolov5 with pytorch to run object detection.  To install pytorch based on your GPU and cuda version, go to the official pytorch website and install with conda command. Make sure you install pytorch &gt;= 1.7.0.  GPU Version highly recommended!</p> <p> Pytorch Official Website </p> <p>The command belows shows an example of installing pytorch v1.8.0 with cuda 11.1 in opencda environment.</p> <pre><code>\nconda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge\n\n</code></pre> <p>After pytorch installation, install the requirements for Yolov5. </p> <pre><code>pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt  # install dependencies\n</code></pre>"},{"location":"md_files/installation/#4-install-sumo-optional","title":"4. Install SUMO (Optional)","text":"<p>SUMO installation is only required for the users who require to conduct co-simulation testing and use future release of SUMO-only mode.</p> <p>You can install SUMO directly by apt-get:</p> <pre><code>sudo add-apt-repository ppa:sumo/stable\nsudo apt-get update\nsudo apt-get install sumo sumo-tools sumo-doc\n</code></pre> <p>After that, install the traci python package.</p> <pre><code>pip install traci\n</code></pre> <p>Finally, add the following path to your ~/.bashrc:</p> <pre><code>export SUMO_HOME=/usr/share/sumo\n</code></pre>"},{"location":"md_files/installation/#5-install-openscenario-optional","title":"5. Install OpenScenario (Optional)","text":"<p>If you want to use OpenScenario to conduct scenario testing, e.g. <code>python opencda.py -t openscenario_carla -v 0.9.12</code>, you need to install OpenScenario first. </p> <p>Please follow the openscenario installation to install OpenScenario.</p>"},{"location":"md_files/installation/#docker-installation","title":"Docker Installation","text":"<p>OpenCDA provides docker image for users to run directly.</p> <p>Note: Openscenario is not supported in docker image yet.</p>"},{"location":"md_files/installation/#1-prerequisite","title":"1. Prerequisite","text":"<p>First, make sure that you have installed <code>docker</code> in your ubuntu system. If you have nvidia gpu, it is recommended to install <code>nvidia-docker</code> as well.</p>"},{"location":"md_files/installation/#2-dockerfile-modification","title":"2. Dockerfile Modification","text":"<p>Next, modify the Dockerfile based on your system. Some parameters you may want to pay attentions are:</p> <ul> <li><code>CARLA_VERSION</code>: Change to the version you want. Notice after(including) OpenCDA 0.1.4, only CARLA &gt;= 0.9.14 will be supported.</li> <li><code>FROM nvidia/vulkan:1.3-470</code>: If you want the carla run with rendering in the docker, make sure the graphics driver is set to the same as your local computer. For instance, mine is <code>FROM nvidia/cuda-11.4.2.0-ase-ubuntu2004</code></li> <li><code>OPENCDA_FULL_INSTALL</code>: If set true, it will run the <code>setup.sh</code> in OpenCDA automatically during building up the docker to install carla api into the python environment. if set to false, then you need to go to your docker container after building to manually run <code>setup.sh</code></li> </ul>"},{"location":"md_files/installation/#3-build-up-the-docker","title":"3. Build up the docker","text":"<pre><code>cd OpenCDA\ndocker build -t opencda_container .\n</code></pre>"},{"location":"md_files/installation/#4-run-the-docker","title":"4. Run the docker","text":"<p>The following command will allow you run Carla with rendering in your docker. If the rendering is the black, then you probably build up the wrong nvidia graphics driver.</p> <pre><code>xhost +local: \n\ndocker run -it --rm \\\n  -e DISPLAY=$DISPLAY \\\n  -v /tmp/.X11-unix:/tmp/.X11-unix \\\n  -v $HOME/.Xauthority:/root/.Xauthority \\\n  --name opencda_container opencda_docker /bin/bash\n</code></pre>"},{"location":"md_files/introduction/","title":"Introduction","text":""},{"location":"md_files/introduction/#opencda-overview","title":"OpenCDA Overview","text":"<p>Current autonomous driving simulation platforms that support scene rendering and traffic simulation mainly concentrate on single-vehicle intelligence; therefore, developing and testing Cooperative Driving Automation applications (e.g., cooperative perception,  platooning, signalized intersection approach and departure) under a realistic simulated environment becomes difficult.</p> <p>OpenCDA is created to fill such gaps. </p> <p>  OpenCDA 0.1.1 </p>"},{"location":"md_files/introduction/#major-components","title":"Major Components","text":"<p>  1. Cooperative Driving System:  OpenCDA  provides  a  full-stack software   that  contains  the  common  self-driving  modules including     sensing,   planning and actuation  layers ,  and  it  is  developed  purely  in  Python for fast prototyping.     Built  upon these basic modules, OpenCDA supports a range of  common cooperative  driving  applications.   2. Co-Simulation Tools: OpenCDA provides interfaces to integrate multiple open-source simulation tools  with the cooperative driving system. Through the interfaces, OpenCDA is able to take advantage of the high-quality scene rendering   and realistic dynamic modelling from CARLA , and the realistic traffic simulation from SUMO. Also, CARLA only and SUMO only mode of OpenCDA also offers researchers the flexibility to test vehicle-level and traffic-level performance, respectively.</p> <ol> <li> <p>Scenario Manager:  By defining the parameters in the Yaml file,  OpenCDA is able to construct the simulation scenario,  creating the traffic flows, and assigning various dynamic driving tasks  to different connected automated vehicles.   Through such lightweight configuration, researchers can conveniently test and evaluate their algorithms under different scenarios. In the next verision v0.2,   OpenScenario will be supported to trigger special events.</p> </li> <li> <p>CDA Data Manager and Repository: OpenCDA provides a series of practical functions to collect offline CDA data (e.g. V2X perception data, multi-agent trajectory prediction data) and log replay them in the simulator. The recent ICRA work OPV2V comes out from this component. </p> </li> </ol>"},{"location":"md_files/introduction/#key-features","title":"Key Features","text":"<p>The key features of OpenCDA are: * Integration: OpenCDA utilizes CARLA and SUMO separately, as well as integrates them together for realistic scene rendering, vehicle modeling, and traffic simulation. *  Full-stack prototype CDA Platform in Simulation: OpenCDA provides a simple prototype automated driving and cooperative driving platform, all in Python, that contains perception, localization, planning, control, and V2X communication modules. * Modularity: OpenCDA is highly modularized, enabling users to conveniently replace any default algorithms or protocols with their own customzied design.  * Benchmark: OpenCDA offers benchmark testing scenarios, benchmark baseline maps, state-of-the-art benchmark algorithms for ADS and Cooperative ADS functions, and benchmark evaluation metrics. * Connectivity and Cooperation: OpenCDA supports various levels and categories of cooperation between CAVs in simulation. This differentiates OpenCDA from other single vehicle simulation tools.</p>"},{"location":"md_files/logic_flow/","title":"Logic flow","text":""},{"location":"md_files/logic_flow/#logic-flow","title":"Logic Flow","text":"<p>In this section,  we will introduce the logic flow of conducting a  scenario test in OpenCDA.</p>"},{"location":"md_files/logic_flow/#codebase-structure","title":"Codebase Structure","text":"<p>Check the codebase structure to see how the codes distributed in OpenCDA.</p>"},{"location":"md_files/logic_flow/#overview","title":"Overview","text":"<p>As the figure below depicts, to run simulation testings in OpenCDA, there are five general steps:</p> <ol> <li>The user has to first write a yaml file to configure the settings of simulation server (e.g. sync mode vs async mode),  the specifications of the traffic flow (e.g. the number of human drive vehicles, spawn positions), and the parameters of  each Connected Automated  Vehicle (e.g. lidar parameters, detection model, trajectory smoothness).</li> <li>The  Scenario Manager  will load the yaml file, and deliver the necessary information to CARLA server to set simulation setting, create traffic flow and generate the CAVs. Each CAV is managed by a class called  <code>VehicleManager</code>.</li> <li>The simulation server information will be passed to each <code>VehicleManager</code>. Based on whether the corresponding cooperative modules are activated, the <code>VehicleManager</code> will select different perception, localization, and planning modules to send the planned trajectory to the <code>ControlManager</code>. The controller will produce control commands and deliver to the  simulation server.</li> <li>The simulation server will apply the received control commands to the vehicles, execute a single step, and return the updated information to the <code>VehicleManager</code> for next round running.</li> <li>After simulation is over, <code>EvaluaitonManager</code> will evaluate different modules' performance and save the statistics.</li> </ol> <p></p>"},{"location":"md_files/logic_flow/#step1-define-the-yaml-file","title":"Step1: Define the yaml file","text":"<p>Check the Yaml Define Rule to see how to write a yaml file to define your scenario.</p>"},{"location":"md_files/logic_flow/#step2-construct-scenario-carla-only","title":"Step2: Construct scenario (CARLA only)","text":"<p>If the simulation only requires CARLA simulator, then after the yaml file is given, the Scenario Manager will load the file and construct the scenario through <code>opencda.sim_api</code>. </p> <p>The users need to first load the yaml file into a dictionary, and initialize the <code>ScenarioManager</code>.</p> <pre><code>import opencda.scenario_testing.utils.sim_api as sim_api\n\n# Aad yaml file into a dictionary\nscenario_params = load_yaml(config_yaml)\n\n# Create CAV world object to store all CAV VehicleManager info.\n# this is the key element to achieve cooperation\ncav_world = CavWorld(opt.apply_ml)\n\n# create scenario manager\nscenario_manager = sim_api.ScenarioManager(scenario_params,\n                                           opt.apply_ml,\n                                           town='Town06',\n                                           cav_world=cav_world)\n</code></pre> <p>Afterwards, the platoons and single CAVs will be generated.</p> <pre><code># create a list of platoon\nplatoon_list = scenario_manager.create_platoon_manager(\n        map_helper=map_api.spawn_helper_2lanefree,\n        data_dump=False)\n\n# create a list of single CAV\nsingle_cav_list = scenario_manager.create_vehicle_manager(application=['single'])\n</code></pre> <p>Next, the traffic flow is prodced. Check CARLA Traffic Generation to see more details about CARLA traffic generation.</p> <pre><code># create background traffic under Carla\ntraffic_manager, bg_veh_list = scenario_manager.create_traffic_carla()\n</code></pre> <p>Finally, create the <code>EvaluationManager</code></p> <pre><code>from opencda.scenario_testing.evaluations.evaluate_manager import EvaluationManager\neval_manager = \\\n    EvaluationManager(scenario_manager.cav_world,\n                      script_name='platoon_joining_town06_carla',\n                      current_time=scenario_params['current_time'])\n</code></pre>"},{"location":"md_files/logic_flow/#step2-construct-scenario-co-simulation","title":"Step2: Construct scenario (Co-Simulation)","text":"<p>Constructing a scenario under co-simulation setting is very similar with building scenario  in CARLA only. There are only two differences: 1) Co-simulation requires addtional Sumo files. 2) Instead of using <code>ScenarioManager</code>, <code>CoScenarioManager</code> is used to control the traffic. Check Traffic Generation under Sumo section to see more details.</p> <pre><code>import opencda.scenario_testing.utils.cosim_api as sim_api\n\n# there should be a Town06.sumocfg, a Town06.net.xml, and a Town06.rou.xml in\n# Town06 folder\nsumo_cfg = 'Town06'\n\n# create co-simulation scenario manager\nscenario_manager = \\\n    sim_api.CoScenarioManager(scenario_params,\n                              opt.apply_ml,\n                               town='Town06',\n                              cav_world=cav_world,\n                              sumo_file_parent_path=sumo_cfg)\n</code></pre>"},{"location":"md_files/logic_flow/#step3-execute-a-single-step","title":"Step3: Execute a single step","text":"<p>A simplified class diagram design is shown below. The core class in OpenCDA is <code>VehicleManager</code>, which is the base class for any cooperative driving applications (e.g. <code>PlatoonManager</code> is built upon <code>VehicleManager</code> ). It contains the necessary modules such as <code>PerceptionManager</code> and <code>LocalizationManager</code>.</p> <p></p> <p>Based on whether certain cooperative driving application is activated, <code>VehicleManager</code> will choose different perception/localization/planning manager.</p> <pre><code># vehicle_manager.py\nclass VehicleManager:\n    def __init__(self, vehicle, config_yaml, application, carla_map, cav_world):\n        if 'platooning' in application:\n            platoon_config = config_yaml['platoon']\n            self.agent = PlatooningBehaviorAgent(vehicle, self, self.v2x_manager,\n                                                 behavior_config, platoon_config, carla_map)\n        else:\n            self.agent = BehaviorAgent(vehicle, carla_map, behavior_config)\n\n</code></pre> <p>During runtime, <code>VehicleManager</code> will first localize and detect the surrounding objects, and then pass the computed information to v2x stack, planner and controller. Then the donwstream modules will fuse information from different CAVs, generate trajectory and control commands.</p> <pre><code>class VehicleManager:\n    def update_info(self):\n        # localization\n        self.localizer.localize()\n        ego_pos = self.localizer.get_ego_pos()\n        ego_spd = self.localizer.get_ego_spd()\n\n        # object detection\n        objects = self.perception_manager.detect(ego_pos)\n\n        self.v2x_manager.update_info(ego_pos, ego_spd)\n        self.agent.update_information(ego_pos, ego_spd, objects)\n        # pass position and speed info to controller\n        self.controller.update_info(ego_pos, ego_spd)\n\n    def run_step(self, target_speed=None):\n        target_speed, target_pos = self.agent.run_step(target_speed)\n        control = self.controller.run_step(target_speed, target_pos)\n        return control\n\n</code></pre>"},{"location":"md_files/logic_flow/#step4-keep-the-simulation-loop-running","title":"Step4: Keep the simulation loop running","text":"<pre><code>while True:\n    world.tick()\n    single_cav.update_info()\n    control = single_cav.run_step()\n    single_cav.vehicle.apply_control(control)\n</code></pre>"},{"location":"md_files/logic_flow/#step5-evaluation","title":"Step5: Evaluation","text":"<p>When the simulation is over, the <code>EvaluationManager</code> will evaluate the performance, and save the results in <code>~/OpenCDA/evluation_outputs</code></p> <pre><code># create evaluation manager\neval_manager = EvaluationManager(cav_world)\neval_manager.evaluate()\n</code></pre>"},{"location":"md_files/release_history/","title":"Release history","text":""},{"location":"md_files/release_history/#release-history-of-opencda","title":"Release History of OpenCDA","text":"<p>This page demonstrates all the changes since the origin release v0.1.0 with more detailed imags.</p>"},{"location":"md_files/release_history/#v012","title":"v0.1.2","text":""},{"location":"md_files/release_history/#map-manager","title":"Map manager","text":"<p>OpenCDA now adds a new component <code>map_manager</code> for each cav. It will dynamically load road topology, traffic light information, and dynamic objects information around the ego vehicle and save them into rasterized map, which can be useful for RL planning, HDMap learning, scene understanding, etc. Key elements in the rasterization map: - Drivable space colored by black - Lanes   - Red lane: the lanes that are controlled by red traffic light   - Green lane: the lanes that are controlled by green traffic light   - Yellow lane: the lanes that are not effected by any traffic light - Objects that are colored by white and represented as rectangle</p> <p></p>"},{"location":"md_files/release_history/#v011","title":"v0.1.1","text":""},{"location":"md_files/release_history/#cooperative-perception","title":"Cooperative Perception","text":"<p>OpenCDA now supports data dumping simultaneously for multiple CAVs to develop V2V perception  algorithms offline. The dumped data includes:  - LiDAR data - RGB camera (4 for each CAV) - GPS/IMU - Velocity and future planned trajectory of the CAV - Surrounding vehicles' bounding box position, velocity </p> <p>Besides the above dumped data, users can also generate the future trajectory for each  vehicle for trajectory prediction purpose. Run <code>python root_of_opencda/scripts/generate_prediction_yaml.py</code> to generate the prediction offline.</p> <p>This new functionality has been proved helpful. The newest ICRA 2022 paper OPV2V: An Open Benchmark Dataset and Fusion Pipeline for Perception with Vehicle-to-Vehicle Communication has utilized this new feature to collect cooperative data. Check https://mobility-lab.seas.ucla.edu/opv2v/ for more information</p> <p></p>"},{"location":"md_files/release_history/#carla-0912-support","title":"CARLA 0.9.12 Support","text":"<p>OpenCDA now supports both CARLA 0.9.12 and 0.9.11. Users needs to set CARLA_VERSION variable before installing OpenCDA. When users run opencda.py, -v argument is required to classify the CARLA version for OpenCDA to select the correct API.</p>"},{"location":"md_files/release_history/#weather-parameters","title":"Weather Parameters","text":"<p>To help estimate the influence of weather on cooperative driving automation, users now can  define weather setting in the yaml file to control sunlight, fog, rain, wetness and other conditions.</p>"},{"location":"md_files/release_history/#bug-fixes","title":"Bug Fixes","text":"<p>Some minor bugs in the planning module are fixed.</p>"},{"location":"md_files/traffic_generation/","title":"Traffic generation","text":""},{"location":"md_files/traffic_generation/#traffic-generation","title":"Traffic Generation","text":"<p>OpenCDA supports two different ways to generate the background traffic flow: CARLA traffic manager and SUMO traffic simulation. The traffic and other scenario generation function are currently being enhanced and will be released in the future.</p>"},{"location":"md_files/traffic_generation/#carla-traffic-manager","title":"CARLA Traffic Manager","text":"<p>OpenCDA support researchers with the CARLA built-in traffic manager to control the traffic flow. All the test scripts that utilize CARLA traffic manager should have names ending with 'carla', e.g. single_2lanefree_carla.py, single_town06_carla.py.</p> <p>To generate the CARLA traffic flow, users need to define the corresponding parameters in yaml file  and call the APIs in <code>opencda.scenario_testing.utils.sim_api.ScenarioManager</code>: * Check the carla_traffic_manager section in yaml rule to see how to define parameters related to carla traffic in the yaml file. * Utilizing <code>ScenarioManager</code> to generate CARLA traffic flow is easy. It just takes 3 lines of codes. <code>scenario_manager.tick()</code> will keep the traffic manager keep running during the simulation loop.</p> <pre><code>import opencda.scenario_testing.utils.sim_api as sim_api\n# scenario_params are defined in the yaml file\nscenario_manager = sim_api.ScenarioManager(scenario_params,\n                                           opt.apply_ml,\n                                           xodr_path=xodr_path,\n                                           cav_world=cav_world)\n# create background traffic in carla\ntraffic_manager, bg_veh_list = scenario_manager.create_traffic_carla()\n\nwhile True:\n    scenario_manager.tick()\n\n</code></pre>"},{"location":"md_files/traffic_generation/#sumo-traffic-management-co-simulation","title":"Sumo Traffic Management (Co-Simulation)","text":"<p>OpenCDA provides the interface to enable users control the CAVs in CARLA  and manage the traffic flow in Sumo. All the test scripts that utilize Sumo traffic manager should have names ending with 'cosim', e.g. single_2lanefree_cosim.py, single_town06_cosim.py.</p> <p>To generate the Sumo traffic flow, three things are needed: * Define the Sumo server settings in yaml file. Check our yaml rule sumo part to see more details. * Three files that defines Sumo road network and traffic route: a <code>xxx.sumocfg</code> file,    a <code>xxx.net.xml</code> file, and a <code>xxx.rou.xml</code> file. xxx is the name of the testing map, e,g. Town06. This   name should keep consistent acroos the three files.</p> <pre><code>*   `.sumocfg` : used to give the Sumo server the path of the network and route xml file. Check\n    [.sumocfg file extension](https://fileinfo.com/extension/sumocfg) to see more explanations.\n* `.net.xml` : defines the road graph. Check [Sumo Road Networks](https://sumo.dlr.de/docs/Networks/SUMO_Road_Networks.html)\n  to see more details. This xml file can be converted from your xodr file:\n</code></pre> <p><code>bash   cd root/of/OpenCDA   python scripts/netconvert_carla.py your_map.xodr -o your_map.net.xml</code>    * <code>rou.xml</code> : defines the traffic flow. As the example presents below, <code>vType</code> is used to define      the vehicle types and <code>flow</code> defines the route. In this example, the server will generate continuous traffic flow      in the first and second lanes of the road <code>-63</code>. Both lanes' vehicles will try to reach road <code>-62</code> via road <code>-61</code> with      a speed of 15 m/s. To know more details about the parameters' meaning in route file, check Sumo Vehicle and Routes.   ```xml    <p></p> <p><code>`` * Use</code>CoScenarioManager<code>to load sumo files and create sumo traffic flow. During initialization,</code>CoscenarioManager<code>will setup the Sumo server. During the</code>tick()<code>function,</code>CoScenarioManager`    will keep spawning Sumo vechiles as traffic flow.</p> <pre><code>```python\n</code></pre> <p>import opencda.scenario_testing.utils.cosim_api as sim_api</p> <p># there should be a Town06.sumocfg, a Town06.net.xml, and a Town06.rou.xml in   # Town06 folder   sumo_cfg = 'Town06'</p> <p># create co-simulation scenario manager   scenario_manager = \\   sim_api.CoScenarioManager(scenario_params,                             opt.apply_ml,                             town='Town06',                             cav_world=cav_world,                             sumo_file_parent_path=sumo_cfg)</p> <p>while True:         scenario_manager.tick()     ```</p>"},{"location":"md_files/yaml_define/","title":"Yaml define","text":""},{"location":"md_files/yaml_define/#yaml-rule","title":"Yaml Rule","text":"<p>To create a scenario test in OpenCDA, you need to start by writing a yml file to define the simulation parameters. This can be a bit tricky, so we've provided a helpful starting point in the form of a yaml file called default.yaml.</p> <p>The default.yaml file provides default parameters for a scenario, which you can modify as needed to create your own scenario. Instead of starting from scratch, you can use this file as a template and only change the parts that are different from the default parameters.</p> <p>If you're not sure where to start, we've included example yaml files for various scenarios in the opencda/scenario_testing/config_yaml directory. You can use these as a guide or a starting point for your own scenario.</p> <p>Below show an concrete example:</p> <pre><code># default.yaml\ndescription: |-\n  Copyright 2021 &lt;UCLA Mobility Lab&gt;\n  Author: Runsheng Xu &lt;rxx3386@ucla.edu&gt;\n  Content: This is the template scenario testing configuration file that other scenarios could directly refer\n\n# define carla simulation setting\nworld:\n  sync_mode: true\n  client_port: 2000\n  fixed_delta_seconds: 0.05\n  seed: 11 # seed for numpy and random\n  weather:\n    sun_altitude_angle: 15 # 90 is the midday and -90 is the midnight\n    cloudiness: 0 # 0 is the clean sky and 100 is the thickest cloud\n    precipitation: 0 # rain, 100 is the heaviest rain\n    precipitation_deposits: 0 # Determines the creation of puddles. Values range from 0 to 100, being 0 none at all and 100 a road completely capped with water.\n    wind_intensity: 0 # it will influence the rain\n    fog_density: 0 # fog thickness, 100 is the largest\n    fog_distance: 0  # Fog start distance. Values range from 0 to infinite.\n    fog_falloff: 0 # Density of the fog (as in specific mass) from 0 to infinity. The bigger the value, the more dense and heavy it will be, and the fog will reach smaller heights\n    wetness: 0\n\n\n# Define the basic parameters of the rsu\nrsu_base:\n  sensing:\n    perception:\n      activate: false # when not activated, objects positions will be retrieved from server directly\n      camera:\n        visualize: 4 # how many camera images need to be visualized. 0 means no visualization for camera\n        num: 4 # how many cameras are mounted on the vehicle. Maximum 3(frontal, left and right cameras)\n        # relative positions (x,y,z,yaw) of the camera. len(positions) should be equal to camera num\n        positions:\n          - [2.5, 0, 1.0, 0]\n          - [0.0, 0.3, 1.8, 100]\n          - [0.0, -0.3, 1.8, -100]\n          - [-2.0, 0.0, 1.5, 180]\n      lidar: # lidar sensor configuration, check CARLA sensor reference for more details\n        visualize: true\n        channels: 32\n        range: 120\n        points_per_second: 1000000\n        rotation_frequency: 20 # the simulation is 20 fps\n        upper_fov: 2\n        lower_fov: -25\n        dropoff_general_rate: 0.3\n        dropoff_intensity_limit: 0.7\n        dropoff_zero_intensity: 0.4\n        noise_stddev: 0.02\n    localization:\n      activate: true # when not activated, ego position will be retrieved from server directly\n      dt: ${world.fixed_delta_seconds} # used for kalman filter\n      gnss: # gnss sensor configuration\n        noise_alt_stddev: 0.05\n        noise_lat_stddev: 3e-6\n        noise_lon_stddev: 3e-6\n\n# Basic parameters of the vehicles\nvehicle_base:\n  sensing: # include perception and localization\n    perception:\n      activate: false # when not activated, objects positions will be retrieved from server directly\n      camera:\n        visualize: 1 # how many camera images need to be visualized. 0 means no visualization for camera\n        num: 1 # how many cameras are mounted on the vehicle.\n        positions:  # relative positions (x,y,z,yaw) of the camera. len(positions) should be equal to camera num\n          - [2.5, 0, 1.0, 0]\n      lidar: # lidar sensor configuration, check CARLA sensor reference for more details\n        visualize: true\n        channels: 32\n        range: 50\n        points_per_second: 100000\n        rotation_frequency: 20 # the simulation is 20 fps\n        upper_fov: 10.0\n        lower_fov: -30.0\n        dropoff_general_rate: 0.0\n        dropoff_intensity_limit: 1.0\n        dropoff_zero_intensity: 0.0\n        noise_stddev: 0.0\n\n    localization:\n      activate: false # when not activated, ego position will be retrieved from server directly\n      dt: ${world.fixed_delta_seconds} # used for kalman filter\n      gnss: # gnss sensor configuration\n        noise_alt_stddev: 0.001\n        noise_lat_stddev: 1.0e-6\n        noise_lon_stddev: 1.0e-6\n        heading_direction_stddev: 0.1 # degree\n        speed_stddev: 0.2\n      debug_helper:\n        show_animation: false # whether to show real-time trajectory plotting\n        x_scale: 1.0 # used to multiply with the x coordinate to make the error on x axis clearer\n        y_scale: 100.0 # used to multiply with the y coordinate to make the error on y axis clearer\n\n  map_manager:\n    pixels_per_meter: 2 # rasterization map resolution\n    raster_size: [224, 224] # the rasterize map size (pixel)\n    lane_sample_resolution: 0.1 # for every 0.1m, we draw a point of lane\n    visualize: true # whether to visualize the rasteraization map\n    activate: true # whether activate the map manager\n\n  safety_manager: # used to watch the safety status of the cav\n    print_message: true # whether to print the message if hazard happens\n    collision_sensor:\n      history_size: 30\n      col_thresh: 1\n    stuck_dector:\n      len_thresh: 500\n      speed_thresh: 0.5\n    offroad_dector: [ ]\n    traffic_light_detector: # whether the vehicle violate the traffic light\n      light_dist_thresh: 20\n\n  behavior:\n    max_speed: 111 # maximum speed, km/h\n    tailgate_speed: 121 # when a vehicles needs to be close to another vehicle asap\n    speed_lim_dist: 3 # max_speed - speed_lim_dist = target speed\n    speed_decrease: 15 # used in car following mode to decrease speed for distance keeping\n    safety_time: 4 # ttc safety thresholding for decreasing speed\n    emergency_param: 0.4 # used to identify whether a emergency stop needed\n    ignore_traffic_light: true # whether to ignore traffic light\n    overtake_allowed: true # whether overtake allowed, typically false for platoon leader\n    collision_time_ahead: 1.5 # used for collision checking\n    overtake_counter_recover: 35 # the vehicle can not do another overtake during next certain steps\n    sample_resolution: 4.5 # the unit distance between two adjacent waypoints in meter\n    local_planner:  # trajectory planning related\n      buffer_size: 12 # waypoint buffer size\n      trajectory_update_freq: 15 # used to control trajectory points updating frequency\n      waypoint_update_freq: 9 # used to control waypoint updating frequency\n      min_dist: 3 # used to pop out the waypoints too close to current location\n      trajectory_dt: 0.20 # for every dt seconds, we sample a trajectory point from the trajectory path as next goal state\n      debug: false # whether to draw future/history waypoints\n      debug_trajectory: false # whether to draw the trajectory points and path\n\n  controller:\n    type: pid_controller # this has to be exactly the same name as the controller py file\n    args:\n      lat:\n        k_p: 0.75\n        k_d: 0.02\n        k_i: 0.4\n      lon:\n        k_p: 0.37\n        k_d: 0.024\n        k_i: 0.032\n      dynamic: false # whether use dynamic pid setting\n      dt: ${world.fixed_delta_seconds} # this should be equal to your simulation time-step\n      max_brake: 1.0\n      max_throttle: 1.0\n      max_steering: 0.3\n  v2x:  # communication related\n    enabled: true\n    communication_range: 35\n\n\n# define the background traffic control by carla\ncarla_traffic_manager:\n  sync_mode: true # has to be same as the world setting\n  global_distance: 5 # the minimum distance in meters that vehicles have to keep with the rest\n  # Sets the difference the vehicle's intended speed and its current speed limit.\n  #  Carla default speed is 30 km/h, so -100 represents 60 km/h,\n  # and 20 represents 24 km/h\n  global_speed_perc: -100\n  set_osm_mode: true # Enables or disables the OSM mode.\n  auto_lane_change: false\n  ignore_lights_percentage: 0 # whether set the traffic ignore traffic lights\n  random: false # whether to random select vehicles' color and model\n  vehicle_list: []  # define in each scenario. If set to ~, then the vehicles be spawned in a certain range\n  # Used only when vehicle_list is ~\n  # x_min, x_max, y_min, y_max, x_step, y_step, vehicle_num\n  range: []\n\n# define the platoon basic characteristics\nplatoon_base:\n  max_capacity: 10\n  inter_gap: 0.6 # desired time gap\n  open_gap: 1.2 # open gap\n  warm_up_speed: 55 # required speed before cooperative merging\n  change_leader_speed: true # whether to assign leader multiple speed to follow\n  leader_speeds_profile: [ 85, 95 ] # different speed for leader to follow\n  stage_duration: 10 # how long should the leader keeps in the current velocity stage\n\n# define tne scenario in each specific scenario\nscenario:\n  single_cav_list: []\n  platoon_list: []\n</code></pre> <p>The above yaml file is the <code>default.yaml</code>. If the users wants to create a platoon joining scenario in highway, here is how we create <code>platoon_joining_2lanefree_carla.yaml</code>:</p> <pre><code># platoon_joining_2lanefree_carla.yaml\nvehicle_base:\n  sensing:\n    perception:\n      camera:\n        visualize: 0 # how many camera images need to be visualized. 0 means no visualization for camera\n        num: 0 # how many cameras are mounted on the vehicle. Maximum 3(frontal, left and right cameras)\n        # relative positions (x,y,z,yaw) of the camera. len(positions) should be equal to camera num\n        positions: []\n      lidar:\n        visualize: false\n  map_manager:\n    visualize: false\n    activate: false\n  behavior:\n    max_speed: 95 # maximum speed, km/h\n    tailgate_speed: 105 # when a vehicles needs to be close to another vehicle asap\n    overtake_allowed: false # whether overtake allowed, typically false for platoon leader\n    collision_time_ahead: 1.3 # used for collision checking\n    overtake_counter_recover: 35 # the vehicle can not do another overtake during next certain steps\n    local_planner:\n      trajectory_dt: 0.25 # for every dt seconds, we sample a trajectory point from the trajectory path as next goal state\n\n# define the platoon basic characteristics\nplatoon_base:\n  max_capacity: 10\n  inter_gap: 0.6 # desired time gap\n  open_gap: 1.5 # open gap\n  warm_up_speed: 55 # required speed before cooperative merging\n\n# define the background traffic control by carla\ncarla_traffic_manager:\n  global_distance: 4.0 # the minimum distance in meters that vehicles have to keep with the rest\n  # Sets the difference the vehicle's intended speed and its current speed limit.\n  #  Carla default speed is 30 km/h, so -100 represents 60 km/h,\n  # and 20 represents 24 km/h\n  global_speed_perc: -300\n  vehicle_list:\n    - spawn_position: [-285, 8.3, 0.3, 0, 0, 0]\n    - spawn_position: [-310, 8.3, 0.3, 0, 0, 0]\n    - spawn_position: [-390, 8.3, 0.3, 0, 0, 0]\n    - spawn_position: [-320, 4.8, 0.3, 0, 0, 0]\n      vehicle_speed_perc: -200\n    - spawn_position: [-335, 4.8, 0.3, 0, 0, 0]\n    - spawn_position: [-360, 4.8, 0.3, 0, 0, 0]\n    - spawn_position: [-400, 4.8, 0.3, 0, 0, 0]\n    - spawn_position: [-410, 4.8, 0.3, 0, 0, 0]\n\n# define scenario. In this scenario, a 4-vehicle platoon already exists.\nscenario:\n  platoon_list:\n    - name: platoon1\n      destination: [1000.372955, 8.3, 0.3]\n      members: # the first one is regarded as leader by default\n        - name: cav1\n          spawn_position: [-350, 8.3, 0.3, 0, 0, 0] # x, y, z, roll, yaw, pitch\n          perception:\n            camera:\n              visualize: 1 # how many camera images need to be visualized. 0 means no visualization for camera\n              num: 1 # how many cameras are mounted on the vehicle. Maximum 3(frontal, left and right cameras)\n              # relative positions (x,y,z,yaw) of the camera. len(positions) should be equal to camera num\n              positions:\n                - [2.5, 0, 1.0, 0]\n            lidar:\n              visualize: true\n          behavior:\n            local_planner:\n              debug_trajectory: true\n              debug: false\n        - name: cav2\n          spawn_position: [-360, 8.3, 0.3, 0, 0, 0]\n        - name: cav3\n          spawn_position: [-370, 8.3, 0.3, 0, 0, 0]\n        - name: cav4\n          spawn_position: [-380, 8.3, 0.3, 0, 0, 0]\n  single_cav_list: # this is for merging vehicle or single cav without v2x\n    - name: single_cav\n      spawn_position: [-380, 4.8, 0.3, 0, 0, 0]\n      # when this is defined, the above parameter will be ignored, and a special map function will\n      # be used to define the spawn position based on the argument\n      spawn_special: [0.625]\n      destination: [300, 12.0, 0]\n      sensing:\n        perception:\n          camera:\n            visualize: 1 # how many camera images need to be visualized. 0 means no visualization for camera\n            num: 1 # how many cameras are mounted on the vehicle. Maximum 3(frontal, left and right cameras)\n            # relative positions (x,y,z,yaw) of the camera. len(positions) should be equal to camera num\n            positions:\n              - [2.5, 0, 1.0, 0]\n          lidar:\n            visualize: true\n      v2x:\n        communication_range: 35\n      behavior:\n        overtake_allowed: true\n        local_planner:\n          debug_trajectory: true\n          debug: false\n</code></pre> <p>As you can see, the <code>platoon_joining_2lanefree_carla.yaml</code> only contains the part that <code>default.yaml</code> does not have or has different parameters. </p>"},{"location":"md_files/yaml_define/#detailed-explanation","title":"Detailed Explanation","text":""},{"location":"md_files/yaml_define/#world","title":"world","text":"<p>The parameter <code>world</code> in the yaml defines the CARLA server setting. * <code>sync</code> : boolean type, if true, the simulation will be in sync mode, otherwise async mode. Check  the CARLA Sync documentation  to know more. * <code>client_port</code> : the python client port connected to the CARLA server. * <code>fixed_delta_seconds</code> : The elapsed time remains constant between simulation steps.  If it is set to 0.05 seconds, there will be 20 frames per simulated second.</p> <p>#### vehicle_base  <code>vehicle_base</code> defines the default parameters for each CAV including perception, localization, planning, control,  and v2x modules. The ampersand <code>&amp;</code> character before <code>vehicle_base</code> is ued to to create a <code>named anchor</code>,   which be referenced later on with an asterisk <code>*</code>. </p> <ul> <li><code>sensing</code> :  Define perception and localization related parameters.<ul> <li><code>perception</code>:  Module related to object detection.<ul> <li><code>activate</code> : bool type, if false, the CAV will retrieve object positions from the server directly.  Otherwise, yolov5 will be used for object detection.</li> <li><code>camera_visualize</code> : int type, indicates how many camera rgb images should be visualized during simulation. 0 means no rgb image visualization.</li> <li><code>camera_num</code> : int type, indicates how many cameras are mounted in the CAV, e.g. 3 represents a frontal, a left, and a right camera will be mounted on the top of the vehicle to cover a 270 degree of FOV.</li> <li><code>lidar_visualize</code> : bool type, indicates whether to visualize 3d lidar points during simulaiton.</li> <li><code>lidar</code> : set the configuration of the lidar sensor.</li> </ul> </li> <li><code>localization</code> : Module related to self-localization.<ul> <li><code>activate</code> : bool type, if true, the CAV will use gnss+imu+kf to get ego vehicle position. Otherwise,  the CAV will load the ego position from server directly.</li> <li><code>gnss</code> : related to the parameters of the gnss sensor.</li> <li><code>debug_helper</code> : parameters related to localization debugging and real-time trajectory plotting.</li> </ul> </li> </ul> </li> <li><code>map_manager</code> : Define HDMap manager parameters</li> <li><code>pixels_per_meter</code> : The rasterization map precision.</li> <li><code>raster_size</code> : The rasterization map's H and W in pixels.</li> <li><code>lane_sample_resolution</code> : Waypoint sampling resolution for lane drawing.</li> <li><code>visualize</code> : Whether to show the rasterization map during running in real-time.</li> <li><code>activate</code> : Whether activate the map_manager module.</li> <li><code>behavior</code> : Define behavior planning parameters<ul> <li><code>max_speed</code> : int type, the maximum speed (km/h) that the CAV is allowed to reach.</li> <li><code>tailgate_speed</code> : int type, the target speed (km/h) for CAV when it tries to catch up with a platoon, it is usually larger  than <code>max_speed</code></li> <li><code>speed_lim_dist</code> : int type, during normal driving mode, <code>target_speed</code> = <code>max_speed</code> - <code>speed_lim_dist</code></li> <li><code>speed_decrease</code> : int type, when the CAV is in car following mode and it gets too close to the  front vehicle, <code>target_speed</code> = <code>front_vehicle_speed</code> - <code>speed_decrease</code></li> <li><code>safety_time</code> : float type, ttc thresholding to identify whether the ego vehicle is too close to   the front vehicle.</li> <li><code>emergency_param</code> : float type, <code>emergency_stop_distance</code> = <code>current_speed</code> * <code>emergency_param</code></li> <li><code>ignore_traffic_light</code> : bool type, if set to true, the CAV will ignore the traffic light.</li> <li><code>overtake_allowed</code> :  bool type, if set to false, overtaking is not allowed during driving.</li> <li><code>collision_time_ahead</code> : float type, collision detection range</li> <li><code>sample_resolution</code> : float type, the unit distance (m) between two adjacent waypoints</li> <li><code>local_planner</code> : Define trajectory planning parameters.<ul> <li><code>buffer_size</code> : dequeue type, waypoint buffer size.</li> <li><code>trajectory_update_freq</code> : int type, the update frequency for trajectory, when the length of trajectory buffer   is below the frequency number, the ego vehicle will re-generate the trajectory.</li> <li><code>waypoint_update_freq</code> : int type, the update frequency for waypoint buffer, when the length of the   waypoint buffer is below the frequency, the waypoint buffer will load waypoints from <code>waypoint_queue</code>.</li> <li><code>min_dist</code> : float type, used to pop out the waypoints that are too close to the current location</li> <li><code>trajectory_dt</code> : float type, trajectory points sampling resolution.</li> <li><code>debug</code> : bool type, if true, waypoint will be visualized.</li> <li><code>debug_trajectory</code> : bool type, if true, trajectory will be visualized.</li> </ul> </li> </ul> </li> <li> <p><code>controller</code> : Define controller parameters.</p> <ul> <li><code>type</code> : string type, the type of controller the ego vehicle uses.</li> <li><code>args</code> : the arguments related to the selected controller.</li> </ul> </li> <li> <p><code>v2x</code> : Defome vehicle communication parameters.</p> <ul> <li><code>enabled</code> : bool type, indicate whether v2x is enabled.</li> <li><code>communication_range</code> : float type, the searching range of the CAV</li> <li><code>loc_noise</code> : float type, the deviation of the noise added to the received ego position   during communication.</li> <li><code>yaw_noise</code> : float type, the deviation of the noise added to the received yaw angle  during communication.</li> <li><code>speed_noise</code> : float type, the deviation of the noise added to the received ego speed   during communication.</li> <li><code>lag</code> : int type, the lagging during the communication. E.g., 2 means the CAV   receives the packages of other CAVs at most 2 time steps ago. </li> </ul> </li> </ul>"},{"location":"md_files/yaml_define/#platoon_base","title":"platoon_base","text":"<p><code>platoon_base</code> define the default platooning parameters. * <code>max_capacity</code> : int type, the maximum number of members that the platoon can include. * <code>inter_gap</code> : float type, desired time gap. * <code>open_gap</code> : float type, time gap during cut-in-join. * <code>warm_up_speed</code> : float type, the speed that the merging vehicle needs to reach before do any kind of joining.</p>"},{"location":"md_files/yaml_define/#carla_traffic_manager","title":"carla_traffic_manager","text":"<p><code>carla_traffic_manager</code> defines the traffic flow controlled by CARLA traffic manager.  Users do not need to define this parameter if co-simulation is conducted as Sumo will control the traffic.</p> <p>There are two ways to define the positions of the background vehicles.  * Set the parameter <code>vehicle_list</code> under <code>carla_traffic_manager</code> as a list. An example is demonstrated below. In this example, two vehicles are spawned as background vehicle. The first one is spawned at position <code>x=100, y=100, z=0.3</code>, and the initial rotation angle is <code>roll=0, yaw=20 (degree), pitch=0</code>. The second one is spawned at position  <code>x=122, y=666, z=0.3</code>, and the angle is <code>roll=0, yaw=0, pitch=0</code>.</p> <pre><code>carla_traffic_manager:\n  vehicle_list: \n    - spawn_position: [100, 100, 0.3, 0 , 20, 0]\n    - spawn_position: [122, 666, 0.3, 0 , 0, 0]\n</code></pre> <ul> <li>Set the parameter <code>vehicle_list</code> under <code>carla_traffic_manager</code> as <code>~</code>. The CARLA server will then spawn the vehicles randomly in a certain rectangle range given by the additional parameter <code>range</code>. </li> </ul> <pre><code>carla_traffic_manager:\n  vehicle_list: ~  # a number or a list\n  # Used only when vehicle_list is a number.\n  # x_min, x_max, y_min, y_max, x_step, y_step, veh_num\n  range:\n    - [ 2, 10, 0, 200, 3.5, 25, 30]\n\n</code></pre> <p>Other important parameters: * <code>sync_mode</code> : bool type, it should be consistent with server's sync setting. * <code>global_speed_perc</code> : float type, sets the difference the vehicle's intended speed and its current speed limit.  Speed limits can be exceeded by setting the number to a negative value. Default is 30 km/h. Exceeding a speed limit can be done using negative percentages. For example, -300 will assign a speed of 90, 50 will assign a speed of 15. * <code>auto_lane_change</code> : bool type, whether the vehicles are allowed to do lane change. * <code>random</code> : bool type, if set true, the background traffic will randomly select car model and color. Otherwise, all vehicles will be in lincoln mkz model and green color.</p>"},{"location":"md_files/yaml_define/#scenario","title":"scenario","text":"<p><code>scenario</code> defines each CAV's spawn position and vehicle parameters if different from the default setting <code>vehicle_base</code>.</p> <pre><code>scenario:\n  platoon_list:\n    - &lt;&lt;: *platoon_base\n      destination: [1000.372955, 8.3, 0.3]\n      members: # the first one is regarded as leader by default\n        - &lt;&lt;: *vehicle_base\n          spawn_position: [-350, 8.3, 0.3, 0, 0, 0] # x, y, z, roll, yaw, pitch\n          behavior:\n            &lt;&lt;: *base_behavior\n            overtake_allowed: false\n          platoon: # we need to add platoon specific params\n            &lt;&lt;: *platoon_base\n        - &lt;&lt;: *vehicle_base\n          spawn_position: [-360, 8.3, 0.3, 0, 0, 0]\n          platoon: # we need to add platoon specific params\n            &lt;&lt;: *platoon_base\n   single_cav_list: \n    - &lt;&lt;: *vehicle_base\n      spawn_position: [-380, 4.8, 0.3, 0, 0, 0]\n      destination: [300, 12.0, 0]\n      sensing:\n        &lt;&lt;: *base_sensing\n        perception:\n          &lt;&lt;: *base_perception\n          activate: true\n</code></pre> <p>In the above example, a platoon containing two members and a single CAV that is out of any platoon will be spawn. The <code>destination</code> in the platoon sets the destination of the platoon. The first member of platoon is regarded as the leader by default, and the second member will be the following car. All members in the same platoon should be spawn in the same lane with close distance.  The <code>&lt;&lt;: *vehicle_base</code> will load the whole default setting of CAV  into the leader. However, since overtake is not allowed for a platoon leader and the default setting allows so, this needs to be changed by using the following part:</p> <pre><code>behavior:\n  &lt;&lt;: *base_behavior\n  overtake_allowed: false   \n</code></pre> <p>In this way, the default attribute  <code>overtake_allowed</code> will be overwritten to false while keeping other attributes unchanged. Similarly, the default CAV setting does not have the <code>platoon</code> attribute, thus we also  add <code>platoon: &lt;&lt;: *platoon_base</code> to each member to assign the <code>platoon</code> attribute.</p> <p>For the single CAV, the meaning of the parameters are quite similar with the platoon members we just described.</p>"},{"location":"md_files/yaml_define/#sumo-optional","title":"sumo (optional)","text":"<p><code>sumo</code> needs to be set only when co-simulation is required.</p> <pre><code>sumo:\n  port: ~\n  host: ~\n  gui: true\n  client_order: 1\n  step_length: *delta\n</code></pre> <ul> <li><code>port</code> : int type, TCP port to listen to (default: 8813).</li> <li><code>host</code> : str type, IP of the sumo host server (default: 127.0.0.1).</li> <li><code>gui</code> : bool type, when set to true, Sumo gui will be show.</li> <li><code>client_order</code> : int type, client order number for the co-simulation TraCI connection (default: 1).</li> <li><code>step_length</code> : The elapsed time remains constant between simulation steps. It should be the same as the CARLA server.</li> </ul>"},{"location":"md_files/coperception/setup_scenario_runner/","title":"Setup scenario runner","text":""},{"location":"md_files/coperception/setup_scenario_runner/#scenario-runner","title":"Scenario Runner","text":"<p>To enable the scenario runner, follow the official docs:  https://carla-scenariorunner.readthedocs.io/en/latest/ to set up the environments.</p>"},{"location":"md_files/coperception/setup_scenario_runner/#details-for-each-step","title":"Details for each step:","text":"<ol> <li>Clone the Scenario Runner repo. Find the match version with your installed Carla. We'll use <code>0.9.12</code> as the following example</li> <li>The <code>SCENARIO_RUNNER_ROOT=\"/to/scenario_runner/installation/path\"</code>. In our example, we use <code>${HOME}/scenario_runner-0.9.12</code></li> <li>Set up the environment in <code>.bashrc</code> like</li> </ol> <pre><code>export SCENARIO_RUNNER_ROOT=${HOME}/scenario_runner-0.9.12\nexport PYTHONPATH=$SCENARIO_RUNNER_ROOT:$PYTHONPATH\nexport PYTHONPATH=$PYTHONPATH:${CARLA_ROOT}/PythonAPI/carla/dist/carla-0.9.12-py3.7-linux-x86_64.egg\nexport PYTHONPATH=$PYTHONPATH:${CARLA_ROOT}/PythonAPI/carla\n</code></pre> <ol> <li>source your environment file as <code>source .bashrc</code></li> <li>create an empty <code>__init__.py</code> under the scenario runner repo so that the folder is treated as a package</li> </ol>"},{"location":"md_files/coperception/setup_scenario_runner/#verify-the-installation","title":"Verify the installation","text":"<ol> <li>verify with the import</li> </ol> <pre><code>python -c 'import scenario_runner'\n</code></pre> <ol> <li>Verify with python prompt</li> </ol> <pre><code>&gt;&gt; import scenario_runner as sr\n&gt;&gt; dir(sr)\n['CarlaDataProvider', 'LooseVersion', 'OpenScenario', 'OpenScenarioConfiguration', 'RawTextHelpFormatter', 'RouteParser', 'RouteScenario', 'ScenarioConfigurationParser', 'ScenarioManager', 'ScenarioRunner', 'VERSION', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'argparse', 'carla', 'datetime', 'glob', 'importlib', 'inspect', 'json', 'main', 'os', 'pkg_resources', 'print_function', 'signal', 'sys', 'time', 'traceback']\n</code></pre>"}]}